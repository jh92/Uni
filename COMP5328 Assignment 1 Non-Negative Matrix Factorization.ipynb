{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Negative Matrix Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment will cover Non Negative Matrix Factorisation. Two algorithms using squared Euclidean distance and Kullback-Liebler Divergence cost functions will be built and investigated. Both algorithms will apply multiplicative updates to achieve optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook will use two image datasets - CroppedYaleB and ORL. \n",
    "\n",
    "CroppedYaleB can be found here:\n",
    "http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html\n",
    "\n",
    "and ORL here:\n",
    "http://www.cl.cam.ac.uk/Research/DTG/attarchive:pub/data/att_faces.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "def load_data(root='C:/Users/Harrison/Documents/Uni Work/Advanced Machine Learning/data/CroppedYaleB', reduce=4):\n",
    "    images, labels = [], []\n",
    "    for i, person in enumerate(sorted(os.listdir(root))):\n",
    "        if not os.path.isdir(os.path.join(root, person)):\n",
    "            continue        \n",
    "        for fname in os.listdir(os.path.join(root, person)):  \n",
    "            # Remove background images in Extended YaleB dataset.\n",
    "            if fname.endswith('Ambient.pgm'):\n",
    "                continue            \n",
    "            if not fname.endswith('.pgm'):\n",
    "                continue                \n",
    "            # load image.\n",
    "            img = Image.open(os.path.join(root, person, fname))\n",
    "            img = img.convert('L') # grey image.\n",
    "            # reduce computation complexity.\n",
    "            img = img.resize([s//reduce for s in img.size])\n",
    "            # preprocessing (normalisation)\n",
    "            img = (img-np.min(img))/(np.max(img)-np.min(img))\n",
    "            # convert image to numpy array.\n",
    "            img = np.asarray(img).reshape((-1,1))           \n",
    "            # collect data and label.\n",
    "            images.append(img)\n",
    "            labels.append(i)\n",
    "    # concate all images and labels.\n",
    "    images = np.concatenate(images, axis=1)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "def load_data_without_normalisation(root='C:/Users/Harrison/Documents/Uni Work/Advanced Machine Learning/data/CroppedYaleB', reduce=4):\n",
    "    images, labels = [], []\n",
    "    for i, person in enumerate(sorted(os.listdir(root))):\n",
    "        if not os.path.isdir(os.path.join(root, person)):\n",
    "            continue        \n",
    "        for fname in os.listdir(os.path.join(root, person)):  \n",
    "            # Remove background images in Extended YaleB dataset.\n",
    "            if fname.endswith('Ambient.pgm'):\n",
    "                continue            \n",
    "            if not fname.endswith('.pgm'):\n",
    "                continue                \n",
    "            # load image.\n",
    "            img = Image.open(os.path.join(root, person, fname))\n",
    "            img = img.convert('L') # grey image.\n",
    "            # reduce computation complexity.\n",
    "            img = img.resize([s//reduce for s in img.size])\n",
    "            # preprocessing (normalisation)\n",
    "            img = (img-np.min(img))/(np.max(img)-np.min(img))\n",
    "            # convert image to numpy array.\n",
    "            img = np.asarray(img).reshape((-1,1))           \n",
    "            # collect data and label.\n",
    "            images.append(img)\n",
    "            labels.append(i)\n",
    "    # concate all images and labels.\n",
    "    images = np.concatenate(images, axis=1)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are non negative matrix factorisation algorithms written using standard squared Euclidean distance and KL Divergence cost functions. The multiplicative update rules are written to gaurantee optimisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for NMF using a euclidean based cost function and a multiplicative update algorithm\n",
    "\n",
    "def NMF_euc(V, max_iterations):\n",
    "    n = V.shape[0]\n",
    "    m = V.shape[1]\n",
    "    r = np.linalg.matrix_rank(V)\n",
    "    W = np.random.rand(n, r)\n",
    "    H = np.random.rand(r, m)        \n",
    "    records = []\n",
    "    iteration_count = 0\n",
    "    conv_criteria = []\n",
    "    stop_flag = 0\n",
    "#stop criteria maxes at specified max_iterations or when ratio of the\n",
    "#difference in loss is between 0.99 and 1 for last 20 iterations(i.e. convergence)\n",
    "    while iteration_count < max_iterations and stop_flag != 1:\n",
    "        W_stationary = W\n",
    "        H_stationary = H\n",
    "        top_H = np.dot(W_stationary.T, V)\n",
    "        bottom_H = W_stationary.T.dot(np.dot(W_stationary, H_stationary))\n",
    "        for i in range(H.shape[0]):\n",
    "            for j in range(H.shape[1]):\n",
    "                H[i][j] = H_stationary[i][j] * top_H[i][j] / bottom_H[i][j]  \n",
    "        H_stationary = H\n",
    "        W_stationary = W\n",
    "        top_W = np.dot(V, H_stationary.T)\n",
    "        bottom_W = W_stationary.dot(np.dot(H_stationary, H_stationary.T))\n",
    "        for i in range(W.shape[0]):\n",
    "            for j in range(W.shape[1]):\n",
    "                W[i][j] = W_stationary[i][j] * top_W[i][j] / bottom_W[i][j]\n",
    "        loss = np.sum((V - W.dot(H))**2)   \n",
    "        records.append(loss)\n",
    "        iteration_count += 1\n",
    "        ##print('Iteration %s: %s' %(iteration_count, loss))\n",
    "        if iteration_count > 100:\n",
    "            try:\n",
    "                conv_1 = abs(records[-2] - records[-1])\n",
    "                conv_2 = abs(records[-3] - records[-2])\n",
    "                convergence_value = conv_1/conv_2\n",
    "                conv_criteria.append (convergence_value)\n",
    "                last_20_1 = np.array(conv_criteria[-20:]) > 0.99 \n",
    "                last_20_2 = np.array(conv_criteria[-20:]) < 1  \n",
    "                if np.sum(last_20_1) == np.sum(last_20_2):\n",
    "                    stop_flag = 1\n",
    "                else:\n",
    "                    stop_flag = 0\n",
    "            except:\n",
    "                next\n",
    "    return W,H\n",
    "\n",
    "#KL Divergence NMF\n",
    "def NMF_div(V, max_iterations):\n",
    "    n = V.shape[0]\n",
    "    m = V.shape[1]\n",
    "    r = np.linalg.matrix_rank(V)\n",
    "    W = np.random.rand(n, r)\n",
    "    H = np.random.rand(r, m)        \n",
    "    records = []\n",
    "    iteration_count = 0\n",
    "    conv_criteria = []\n",
    "    stop_flag = 0\n",
    "    while iteration_count < max_iterations and stop_flag != 1:\n",
    "        W_stationary = W\n",
    "        H_stationary = H\n",
    "        top_H = W_stationary.T.dot(V/np.dot(W_stationary, H_stationary))\n",
    "        bottom_H = np.dot(W_stationary.T, np.ones(V.shape))\n",
    "        for i in range(H.shape[0]):\n",
    "            for j in range(H.shape[1]):\n",
    "                H[i][j] = H_stationary[i][j] * top_H[i][j] / bottom_H[i][j]  \n",
    "        H_stationary = H\n",
    "        W_stationary = W\n",
    "        top_W = np.dot((V/np.dot(W_stationary, H_stationary)), H_stationary.T)\n",
    "        bottom_W = np.dot(np.ones(V.shape),H_stationary.T)\n",
    "        for i in range(W.shape[0]):\n",
    "            for j in range(W.shape[1]):\n",
    "                W[i][j] = W_stationary[i][j] * top_W[i][j] / bottom_W[i][j]\n",
    "        z = V/(np.dot(W,H))\n",
    "        z[z<=0] = 0.001\n",
    "        loss = np.sum(V*np.log(z) - V + W.dot(H))\n",
    "        records.append(loss)\n",
    "        iteration_count += 1\n",
    "        #print(loss)\n",
    "        if iteration_count > 100:\n",
    "            try:\n",
    "                conv_1 = abs(records[-2] - records[-1])\n",
    "                conv_2 = abs(records[-3] - records[-2])\n",
    "                convergence_value = conv_1/conv_2\n",
    "                conv_criteria.append (convergence_value)\n",
    "                last_20_1 = np.array(conv_criteria[-20:]) > 0.99 \n",
    "                last_20_2 = np.array(conv_criteria[-20:]) < 1  \n",
    "                if np.sum(last_20_1) == np.sum(last_20_2):\n",
    "                    stop_flag = 1\n",
    "                else:\n",
    "                    stop_flag = 0\n",
    "            except:\n",
    "                next\n",
    "    return W,H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions will be used to measure the robustness of each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robustness evaluation metrics to be used in the experiment\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "def assign_cluster_label(X, Y):\n",
    "    kmeans = KMeans(n_clusters=len(set(Y))).fit(X)\n",
    "    Y_pred = np.zeros(Y.shape)\n",
    "    for i in set(kmeans.labels_):\n",
    "        ind = kmeans.labels_ == i\n",
    "        Y_pred[ind] = Counter(Y[ind]).most_common(1)[0][0] # assign label.\n",
    "    return Y_pred\n",
    "\n",
    "#kmeans = KMeans(n_clusters=len(set(Y))).fit(V_recon.T)\n",
    "\n",
    "#Y_pred = assign_cluster_label(np.dot(W,H).T,Y)\n",
    "#acc = accuracy_score(Y, Y_pred)\n",
    "\n",
    "###Relative reconstruction errors (RRE)\n",
    "#RRE = np.linalg.norm(V-np.dot(W, H)) / np.linalg.norm(V)\n",
    "\n",
    "###NMI\n",
    "#nmi = normalized_mutual_info_score(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise functions\n",
    "\n",
    "def add_gaussian_noise(V, mean, std_dev):\n",
    "    V_noise = np.ones(V.shape)\n",
    "    for column in range(V.shape[1]):\n",
    "        gaussian_noise = np.random.normal(mean, std_dev, V.shape[0])\n",
    "        gaussian_noise[gaussian_noise<0] = 0\n",
    "        V_noise[:,column] = V[:,column] + gaussian_noise        \n",
    "    V_noise[V_noise<0] = 0\n",
    "    return V_noise\n",
    "        \n",
    "def add_laplacian_noise(V, centre, scale):\n",
    "    laplac_noise = np.random.laplace(centre,scale,V.shape)\n",
    "    laplac_noise[laplac_noise<0] = 0\n",
    "    V_noise = V + laplac_noise\n",
    "    return V_noise\n",
    "\n",
    "def add_sparse_noise(V, width, height, reshape_1, reshape_2):\n",
    "    V_noise = []\n",
    "    for image in range(V.shape[1]):\n",
    "        img = V[:, image].reshape(reshape_1,reshape_2)\n",
    "        start_x = np.random.randint(0, reshape_2 - width)\n",
    "        start_y = np.random.randint(0, reshape_1 - height)\n",
    "        img[start_y: start_y+height, start_x:start_x+width] = 1\n",
    "        img = np.asarray(img).reshape((-1,1))\n",
    "        V_noise.append(img)\n",
    "    V_noise = np.concatenate(V_noise, axis=1)\n",
    "    return V_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be described as three separate experiments. Each experiment will add a different type of noise to the dataset (Gaussian, Laplacian and Sparse* respectively) and measure the robustness of the algorithm against RRE, Accuracy and NMI. Within each experiment, the algorithm will be run 5 times and each metric will then be averaged and recorded.\n",
    "\n",
    "Each code will have the optional functions commented out. Feel free to activate them as appropriate. The commented-out code will include:\n",
    "1) Reading in the different datasets (ORL or YALEB)\n",
    "2) Switching between Euclidean or Divergence NMF algorithms\n",
    "3) Adding different types of noise (Gaussian, Laplacian and Sparse)\n",
    "\n",
    "Please refer to report on what sparse noise looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORL dataset: X.shape = (1110, 400), Y.shape = (400,)\n",
      "0.787117189859168\n",
      "0.7905379756182854\n",
      "0.7901003797564998\n",
      "0.8023462138267551\n",
      "0.8016749265455485\n",
      "Results:\n",
      "RRE mean: 0.1088\n",
      "Acc mean: 0.655\n",
      "NMI mean: 0.79436\n"
     ]
    }
   ],
   "source": [
    "#load the ORL Database\n",
    "V, Y = load_data(root='/Users/joshhuang/Desktop/University/Master of Data Science/COMP5328/data/ORL', reduce=3)\n",
    "print('ORL dataset: X.shape = {}, Y.shape = {}'.format(V.shape, Y.shape))  #reshape numbers: 37, 30\n",
    "\n",
    "#load the YaleB Database\n",
    "#V, Y = load_data(root='C:/Users/Harrison/Documents/Uni Work/Advanced Machine Learning/data/CroppedYaleB', reduce=6)\n",
    "#print('Extended YalB dataset: X.shape = {}, Y.shape = {}'.format(V.shape, Y.shape)) #reshape numbers 32, 28\n",
    "\n",
    "rre_results = []\n",
    "acc_results = []\n",
    "nmi_results = []\n",
    "\n",
    "for i in range(5):       \n",
    "    subset = random.sample(range(V.shape[1]), int(0.9*V.shape[1])) #ORL dataset\n",
    "    #subset = random.sample(range(V.shape[1]), int(0.15*V.shape[1])) #YaleB dataset, reduce dataset to reduce computational time\n",
    "    V_test = V[:, subset]\n",
    "    Y_test = Y[subset]\n",
    "        \n",
    "    #adding noise\n",
    "    ##gaussian noise\n",
    "    #V_noise = add_gaussian_noise(V_test, 0, 0.15)\n",
    "    \n",
    "    ##laplacian noise\n",
    "    #V_noise = add_laplacian_noise(V_test, 0, 2)\n",
    "    \n",
    "    ##sparse noise\n",
    "    V_noise = add_sparse_noise(V_test, 10, 5, 37, 30) #if ORL dataset, using reduce = 3\n",
    "    #V_noise = add_sparse_noise(V_test, 10, 5, 32, 28 ) #if YaleB dataset, using reduce = 6\n",
    "    \n",
    "    #apply NMF\n",
    "    W, H = NMF_euc(V_noise, 500)\n",
    "    #W, H = NMF_div(V_noise, 500)\n",
    "    \n",
    "    #evaluation metrics\n",
    "    RRE = np.linalg.norm(V_test-np.dot(W, H)) / np.linalg.norm(V_test)\n",
    "    rre_results.append(RRE)\n",
    "    \n",
    "    Y_pred = assign_cluster_label(np.dot(W,H).T,Y_test)\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    acc_results.append(acc)\n",
    "    \n",
    "    nmi = normalized_mutual_info_score(Y_test, Y_pred)\n",
    "    nmi_results.append(nmi)\n",
    "    print(nmi)\n",
    "    \n",
    "#evaluate results\n",
    "rre_mean = np.round(np.mean(rre_results), 5)\n",
    "acc_mean = np.round(np.mean(acc_results), 5)\n",
    "nmi_mean = np.round(np.mean(nmi_results), 5)\n",
    "print(\"Results:\")\n",
    "print('RRE mean: %s' %(rre_mean))\n",
    "print('Acc mean: %s' %(acc_mean))\n",
    "print('NMI mean: %s' %(nmi_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reconstruct images with noise #ORL datasets\n",
    "image_original = V_subset[:,0].reshape(37,30)\n",
    "plt.imshow(image_original, cmap=plt.cm.gray)\n",
    "\n",
    "noisy_image = V_noise[:,0].reshape(37,30)\n",
    "plt.imshow(noisy_image, cmap=plt.cm.gray)\n",
    "\n",
    "#reconstruct images with noise #YaleB datasets\n",
    "image_original = V_subset[:,0].reshape(32,28) \n",
    "plt.imshow(image_original, cmap=plt.cm.gray)\n",
    "\n",
    "noisy_image = V_noise[:,0].reshape(32,28)\n",
    "plt.imshow(noisy_image, cmap=plt.cm.gray)\n",
    "\n",
    "#applying R\n",
    "r = 300\n",
    "W_final_chop = W_final[:,:r]\n",
    "H_final_chop = H_final[:r,:]\n",
    "recon_r = np.dot(W_final_chop, H_final_chop)\n",
    "noisy_image_r = recon_r[:,0].reshape(37,30)\n",
    "plt.imshow(noisy_image_r, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORL dataset: X.shape = (1110, 400), Y.shape = (400,)\n"
     ]
    }
   ],
   "source": [
    "#noise sampling code\n",
    "mu, sigma = 0.5, 0.16\n",
    "gaussian_noise = np.random.normal(mu, sigma, V_subset.shape[0])\n",
    "hist, bins = np.histogram(gaussian_noise, bins=100, normed=True)\n",
    "bin_centers = (bins[1:]+bins[:-1])*0.5\n",
    "plt.plot(bin_centers, hist)\n",
    "\n",
    "cauchy_noise = np.random.standard_cauchy((V_subset.shape[0]))\n",
    "cauchy_noise.shape\n",
    "hist, bins = np.histogram(cauchy_noise, bins=100, normed=True)\n",
    "bin_centers = (bins[1:]+bins[:-1])*0.5\n",
    "plt.plot(bin_centers, hist)\n",
    "\n",
    "laplac_noise = np.random.laplace(0, 0.2, size = (100,1))\n",
    "hist, bins = np.histogram(laplac_noise, bins=100, normed=True)\n",
    "bin_centers = (bins[1:]+bins[:-1])*0.5\n",
    "plt.plot(bin_centers, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust L1-NMF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was our attempt to create Robust L1-NMF. This was our initial test run before we decided to implement it into a final algorithm. Unfortunately we were unable to get this to converge properly and hence have left out the results from this algorithm from our report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ORL dataset.\n",
    "W, X = load_data(root='/Users/joshhuang/Desktop/University/Master of Data Science/COMP5328/data/ORL', reduce=2)\n",
    "print('ORL dataset: X.shape = {}, Y.shape = {}'.format(W.shape, X.shape))\n",
    "\n",
    "#input_matrix = np.abs(np.random.uniform(low=0.0,high=255.0,size=(100,100)))\n",
    "input_matrix = W\n",
    "S_lambda = 0.5\n",
    "R = 200\n",
    "\n",
    "## Dimensions of the matrix\n",
    "rows, cols = input_matrix.shape\n",
    "W_matrix = np.abs(np.random.uniform(low=0.0,high=255.0,size=(rows, R)))\n",
    "H_matrix = np.abs(np.random.uniform(low=0.0,high=255.0,size=(R, cols)))\n",
    "\n",
    "W_matrix = np.divide(W_matrix, R*W_matrix.max())\n",
    "H_matrix = np.divide(H_matrix, R*H_matrix.max())\n",
    "\n",
    "## Iterations\n",
    "num_iter = 100\n",
    "num_display_cost = max(int(num_iter/10), 1)\n",
    "\n",
    "s_lambda=1.0\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Define new S matrix\n",
    "    S_matrix = input_matrix - np.dot(W_matrix, H_matrix)\n",
    "    \n",
    "    # Regularize the S Matrix\n",
    "    S_matrix = soft_thresholding_operator(S_matrix, S_lambda)\n",
    "    \n",
    "    # Update the W matrix\n",
    "    W_matrix = (np.abs(np.dot((S_matrix - input_matrix),H_matrix.T))\\\n",
    "               - np.dot((S_matrix - input_matrix),H_matrix.T))\\\n",
    "               /(2 * np.dot(np.dot(W_matrix, H_matrix), H_matrix.T))*W_matrix\n",
    "    \n",
    "    # Update the H matrix\n",
    "    H_matrix = (np.abs(np.dot(W_matrix.T,(S_matrix-input_matrix)))\\\n",
    "               - np.dot(W_matrix.T,(S_matrix-input_matrix)))\\\n",
    "               /(2 * np.dot(np.dot(W_matrix.T,W_matrix), H_matrix))*H_matrix\n",
    "    \n",
    "    # Normalize the W and H matrices\n",
    "    W_matrix = W_matrix/np.sqrt(np.sum(W_matrix**2))\n",
    "    H_matrix = H_matrix*np.sqrt(np.sum(W_matrix**2))\n",
    "    \n",
    "    # Calculate the Euclidean cost\n",
    "    c = l1_euclidean_cost(input_matrix, W_matrix, H_matrix, S_matrix)\n",
    "    if i%num_display_cost==0:\n",
    "        print ('Iteration',i,':', c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
