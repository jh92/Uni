{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifar XTR Dimensions:  (10000, 3072)\n",
      "Cifar STR Dimensions:  (10000,)\n",
      "Cifar XTS Dimensions:  (2000, 3072)\n",
      "Cifar YTS Dimensions:  (2000,)\n",
      "\n",
      "Mnist XTR Dimensions:  (10000, 784)\n",
      "Mnist STR Dimensions:  (10000,)\n",
      "Mnist XTS Dimensions:  (2000, 784)\n",
      "Mnist YTS Dimensions:  (2000,)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  0\n",
       "1  1  0\n",
       "2  0  1\n",
       "3  1  0\n",
       "4  1  0\n",
       "5  1  0\n",
       "6  1  0\n",
       "7  1  0\n",
       "8  0  1\n",
       "9  1  0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Basic Libraries and Packages\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, InputLayer, MaxPool2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Dense\n",
    "from skimage.feature import hog\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import np_utils\n",
    "import random\n",
    "\n",
    "# Import Datasets and segment them into components\n",
    "cifar = np.load('/Users/joshhuang/Desktop/University/Master of Data Science/COMP5328/cifar_dataset.npz')\n",
    "cifar_Xtr = cifar['Xtr']\n",
    "cifar_Str = cifar['Str'] \n",
    "cifar_Xts = cifar['Xts']\n",
    "cifar_Yts = cifar['Yts']\n",
    "\n",
    "mnist = np.load('/Users/joshhuang/Desktop/University/Master of Data Science/COMP5328/mnist_dataset.npz')\n",
    "mnist_Xtr = mnist['Xtr']\n",
    "mnist_Str = mnist['Str'] \n",
    "mnist_Xts = mnist['Xts']\n",
    "mnist_Yts = mnist['Yts']\n",
    "\n",
    "# Remove a dimension from the labels\n",
    "cifar_Str = cifar_Str[:,0]\n",
    "cifar_Yts = cifar_Yts[:,0]\n",
    "mnist_Str = mnist_Str[:,0]\n",
    "mnist_Yts = mnist_Yts[:,0]\n",
    "\n",
    "# Data exploration\n",
    "print('Cifar XTR Dimensions: ',cifar_Xtr.shape)\n",
    "print('Cifar STR Dimensions: ',cifar_Str.shape)\n",
    "print('Cifar XTS Dimensions: ',cifar_Xts.shape)\n",
    "print('Cifar YTS Dimensions: ',cifar_Yts.shape)\n",
    "print()\n",
    "print('Mnist XTR Dimensions: ',mnist_Xtr.shape)\n",
    "print('Mnist STR Dimensions: ',mnist_Str.shape)\n",
    "print('Mnist XTS Dimensions: ',mnist_Xts.shape)\n",
    "print('Mnist YTS Dimensions: ',mnist_Yts.shape)\n",
    "\n",
    "# View what the first 10 MNIST train labels look like\n",
    "mnist_df_train = pd.get_dummies(mnist_Str)\n",
    "mnist_df_test = pd.get_dummies(mnist_Yts)\n",
    "cifar_df_train = pd.get_dummies(cifar_Str)\n",
    "cifar_df_test = pd.get_dummies(cifar_Yts)\n",
    "\n",
    "# Reshape data into proper images\n",
    "cifar_train = cifar_Xtr.reshape(10000,32,32,3)\n",
    "cifar_test = cifar_Xts.reshape(2000,32,32,3)\n",
    "\n",
    "mnist_train = mnist_Xtr.reshape(10000,28,28)\n",
    "mnist_test = mnist_Xts.reshape(2000,28,28)\n",
    "\n",
    "print()\n",
    "mnist_df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  1\n",
       "1  1  0\n",
       "2  0  1\n",
       "3  1  0\n",
       "4  1  0\n",
       "5  0  1\n",
       "6  1  0\n",
       "7  1  0\n",
       "8  0  1\n",
       "9  1  0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View what the first 10 MNIST test labels look like\n",
    "mnist_df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  0\n",
       "1  1  0\n",
       "2  0  1\n",
       "3  0  1\n",
       "4  1  0\n",
       "5  1  0\n",
       "6  1  0\n",
       "7  1  0\n",
       "8  1  0\n",
       "9  0  1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View what the first 10 CIFAR training labels look like\n",
    "cifar_df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  1\n",
       "1  0  1\n",
       "2  0  1\n",
       "3  0  1\n",
       "4  0  1\n",
       "5  1  0\n",
       "6  0  1\n",
       "7  0  1\n",
       "8  1  0\n",
       "9  1  0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View what the first 10 CIFAR test labels look like\n",
    "cifar_df_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize histogram of oriented gradients function\n",
    "def transform(image, active=False):\n",
    "    return hog(image, \n",
    "               orientations = 9, \n",
    "               pixels_per_cell = (4, 4), \n",
    "               cells_per_block = (1, 1), \n",
    "               visualise = active, \n",
    "               transform_sqrt = True,\n",
    "               block_norm='L2-Hys')\n",
    "\n",
    "# Create labels out of the probabilities given\n",
    "def probConvert(predictedProb):\n",
    "    for i in range(0, predictedProb.shape[0]):\n",
    "        if predictedProb[i] >= 0.5:\n",
    "            predictedProb[i] = 1\n",
    "        else:\n",
    "            predictedProb[i] = 0\n",
    "    return predictedProb\n",
    "\n",
    "# Assign labels depending on the value\n",
    "def assignLabel(XLabel, YLabel, TransformedLabel):\n",
    "    for i in range(0, XLabel.shape[0]):\n",
    "        if XLabel[i] == YLabel[i]:\n",
    "            TransformedLabel[i] = XLabel[i]\n",
    "        else:\n",
    "            TransformedLabel[i] = random.randint(0,1)\n",
    "    return TransformedLabel\n",
    "\n",
    "# Use this function to obtain the beta coefficient \n",
    "def estimateBeta(train_labels,prob,rho0,rho1):\n",
    "    n = len(train_labels)\n",
    "    beta = np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        if train_labels[i].any()==1:\n",
    "            beta[i] = (prob[i][1]-rho0)/((1-rho0-rho1)*prob[i][1]+1e-5)\n",
    "        else:\n",
    "            beta[i] = (prob[i][0]-rho1)/((1-rho0-rho1)*(prob[i][0])+1e-5)\n",
    "    return beta\n",
    "\n",
    "# Normalize weights\n",
    "def normalise(x):\n",
    "    z = (x - min(x)) / (max(x) - min(x))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare parameters for light gradient boosting\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 10\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshhuang/ve/ve_01/lib/python3.6/site-packages/skimage/feature/_hog.py:248: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Features Train Dimension: (10000, 441)\n",
      "MNIST Features Test Dimension: (2000, 441)\n",
      "CIFAR Features Train Dimension: (10000, 576)\n",
      "CIFAR Features Test Dimension: (2000, 576)\n"
     ]
    }
   ],
   "source": [
    "# Transform mnist train data into HOG features\n",
    "mnist_hog_train = []\n",
    "for i in range(mnist_train.shape[0]): \n",
    "    features_train = transform(mnist_train[i,:,:], active=False)\n",
    "    mnist_hog_train.append(np.array(features_train).flatten())\n",
    "mnist_hog_train = np.array(mnist_hog_train)\n",
    "print(\"MNIST Features Train Dimension:\", mnist_hog_train.shape)\n",
    "\n",
    "# Transform mnist test data into HOG features\n",
    "mnist_hog_test = []\n",
    "for i in range(mnist_test.shape[0]): \n",
    "    features_test = transform(mnist_test[i,:,:], active=False)\n",
    "    mnist_hog_test.append(np.array(features_test).flatten())\n",
    "mnist_hog_test = np.array(mnist_hog_test)\n",
    "print(\"MNIST Features Test Dimension:\", mnist_hog_test.shape)\n",
    "\n",
    "# Transform cifar train data into HOG features\n",
    "cifar_hog_train = []\n",
    "for i in range(cifar_train.shape[0]): \n",
    "    features_train = transform(cifar_train[i,:,:], active=False)\n",
    "    cifar_hog_train.append(np.array(features_train).flatten())\n",
    "cifar_hog_train = np.array(cifar_hog_train)\n",
    "print(\"CIFAR Features Train Dimension:\", cifar_hog_train.shape)\n",
    "\n",
    "# Transform cifar test data into HOG features\n",
    "cifar_hog_test = []\n",
    "for i in range(cifar_test.shape[0]): \n",
    "    features_test = transform(cifar_test[i,:,:], active=False)\n",
    "    cifar_hog_test.append(np.array(features_test).flatten())\n",
    "cifar_hog_test = np.array(cifar_hog_test)\n",
    "print(\"CIFAR Features Test Dimension:\", cifar_hog_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST LGBM Initial Accuracy:  88.9\n",
      "CIFAR LGBM Initial Accuracy:  72.45\n"
     ]
    }
   ],
   "source": [
    "# Train initial classifier on MNIST to compare performance accuracy at the end of the algorithm\n",
    "initLabel = mnist_Str\n",
    "initTrain = mnist_Xtr\n",
    "firstTrain = lgb.Dataset(initTrain, label=initLabel)\n",
    "initClf = lgb.train(params, firstTrain, 1000)\n",
    "initPredict = initClf.predict(mnist_Xts)\n",
    "initConvert = probConvert(initPredict)\n",
    "initAcc = accuracy_score(initConvert, mnist_Yts)\n",
    "print('MNIST LGBM Initial Accuracy: ',initAcc*100)\n",
    "\n",
    "initLabel = cifar_Str\n",
    "initTrain = cifar_Xtr\n",
    "firstTrain = lgb.Dataset(initTrain, label=initLabel)\n",
    "initClf = lgb.train(params, firstTrain, 1000)\n",
    "initPredict = initClf.predict(cifar_Xts)\n",
    "initConvert = probConvert(initPredict)\n",
    "initAcc = accuracy_score(initConvert, cifar_Yts)\n",
    "print('CIFAR LGBM Initial Accuracy: ',initAcc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Iterative Cross Learning LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Classifier X Accuracy:  86.3\n",
      "Epoch 1 Classifier Y Accuracy:  82.35\n",
      "\n",
      "Epoch 2 Classifier X Accuracy:  87.2\n",
      "Epoch 2 Classifier Y Accuracy:  86.55\n",
      "\n",
      "Epoch 3 Classifier X Accuracy:  87.05\n",
      "Epoch 3 Classifier Y Accuracy:  87.25\n",
      "\n",
      "Epoch 4 Classifier X Accuracy:  88.35\n",
      "Epoch 4 Classifier Y Accuracy:  87.0\n",
      "\n",
      "Epoch 5 Classifier X Accuracy:  87.2\n",
      "Epoch 5 Classifier Y Accuracy:  87.6\n",
      "\n",
      "Epoch 6 Classifier X Accuracy:  86.2\n",
      "Epoch 6 Classifier Y Accuracy:  87.6\n",
      "\n",
      "Epoch 7 Classifier X Accuracy:  85.85\n",
      "Epoch 7 Classifier Y Accuracy:  86.95\n",
      "\n",
      "Epoch 8 Classifier X Accuracy:  85.85\n",
      "Epoch 8 Classifier Y Accuracy:  86.2\n",
      "\n",
      "Epoch 9 Classifier X Accuracy:  85.7\n",
      "Epoch 9 Classifier Y Accuracy:  86.05\n",
      "\n",
      "Epoch 10 Classifier X Accuracy:  86.05\n",
      "Epoch 10 Classifier Y Accuracy:  86.15\n",
      "\n",
      "Final Classifier X Average Accuracy:  0.8657499999999999\n",
      "Final Classifier X Standard Deviation:  0.008041299646201462\n",
      "Final Classifier Y Average Accuracy:  0.8637\n",
      "Final Classifier Y Standard Deviation  0.014458907289280199\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "clfX_accuracy_average = []\n",
    "clfY_accuracy_average = []\n",
    "\n",
    "# Implement the iterative cross-learning method\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Extract 80% of the data\n",
    "    initial80, initial20, initial80Label, initial20Label = train_test_split(mnist_hog_train, mnist_Str, test_size=0.2, shuffle = True)\n",
    "    \n",
    "    # Split into another two sets of training data\n",
    "    modelXTrain, modelYTrain, modelXLabel, modelYLabel = train_test_split(initial80, initial80Label, test_size=0.5, shuffle = True)\n",
    "    \n",
    "    # Prepare data for light gradient boosting classification\n",
    "    Xtrain = lgb.Dataset(modelXTrain, label=modelXLabel)\n",
    "    Ytrain = lgb.Dataset(modelYTrain, label=modelYLabel)\n",
    "    \n",
    "    # Train the lgb classifier\n",
    "    clfX = lgb.train(params, Xtrain, 1000)\n",
    "    clfY = lgb.train(params, Ytrain, 1000)\n",
    "    \n",
    "    # Produce a list of probabilities\n",
    "    predictedX = clfX.predict(mnist_hog_train)\n",
    "    predictedY = clfY.predict(mnist_hog_train)\n",
    "    \n",
    "    # Convert the probabilities to a list of labels\n",
    "    XLabel = probConvert(predictedX)\n",
    "    YLabel = probConvert(predictedY)\n",
    "    \n",
    "    # Print the accuracy for each classifier on the test set\n",
    "    testPredictX = clfX.predict(mnist_hog_test)\n",
    "    testXLabel = probConvert(testPredictX)\n",
    "    testPredictY = clfY.predict(mnist_hog_test)\n",
    "    textYLabel = probConvert(testPredictY)\n",
    "    testAccX = accuracy_score(testPredictX, mnist_Yts)\n",
    "    testAccY = accuracy_score(testPredictY, mnist_Yts)\n",
    "    clfX_accuracy_average.append(testAccX)\n",
    "    clfY_accuracy_average.append(testAccY)\n",
    "    \n",
    "    # Assign the labels\n",
    "    mnist_Str = assignLabel(XLabel, YLabel, mnist_Str)\n",
    "    \n",
    "    print('Epoch', i+1, 'Classifier X Accuracy: ', np.round(testAccX*100,2))\n",
    "    print('Epoch', i+1, 'Classifier Y Accuracy: ', np.round(testAccY*100,2))  \n",
    "    print()\n",
    "    \n",
    "    if i == 9:\n",
    "        print('Final Classifier X Average Accuracy: ', np.mean(clfX_accuracy_average))\n",
    "        print('Final Classifier X Standard Deviation: ', np.std(clfX_accuracy_average))\n",
    "        print('Final Classifier Y Average Accuracy: ', np.mean(clfY_accuracy_average))\n",
    "        print('Final Classifier Y Standard Deviation ', np.std(clfY_accuracy_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR Iterative Cross Learning LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Classifier X Accuracy:  70.15\n",
      "Epoch 1 Classifier Y Accuracy:  66.85\n",
      "\n",
      "Epoch 2 Classifier X Accuracy:  62.15\n",
      "Epoch 2 Classifier Y Accuracy:  61.85\n",
      "\n",
      "Epoch 3 Classifier X Accuracy:  58.95\n",
      "Epoch 3 Classifier Y Accuracy:  59.15\n",
      "\n",
      "Epoch 4 Classifier X Accuracy:  57.1\n",
      "Epoch 4 Classifier Y Accuracy:  57.8\n",
      "\n",
      "Epoch 5 Classifier X Accuracy:  56.45\n",
      "Epoch 5 Classifier Y Accuracy:  56.65\n",
      "\n",
      "Epoch 6 Classifier X Accuracy:  56.0\n",
      "Epoch 6 Classifier Y Accuracy:  55.9\n",
      "\n",
      "Epoch 7 Classifier X Accuracy:  55.1\n",
      "Epoch 7 Classifier Y Accuracy:  55.6\n",
      "\n",
      "Epoch 8 Classifier X Accuracy:  54.75\n",
      "Epoch 8 Classifier Y Accuracy:  54.8\n",
      "\n",
      "Epoch 9 Classifier X Accuracy:  54.8\n",
      "Epoch 9 Classifier Y Accuracy:  54.2\n",
      "\n",
      "Epoch 10 Classifier X Accuracy:  53.7\n",
      "Epoch 10 Classifier Y Accuracy:  54.2\n",
      "\n",
      "Final Classifier X Average Accuracy:  0.57915\n",
      "Final Classifier X Standard Deviation 0.046967036312716176\n",
      "Final Classifier Y Average Accuracy:  0.577\n",
      "Final Classifier Y Standard Deviation 0.03804208196195364\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "clfX_accuracy_average = []\n",
    "clfY_accuracy_average = []\n",
    "\n",
    "# Implement the iterative cross-learning method\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Extract 80% of the data\n",
    "    initial80, initial20, initial80Label, initial20Label = train_test_split(cifar_hog_train, cifar_Str, test_size=0.2, shuffle = True)\n",
    "    \n",
    "    # Split into another two sets of training data\n",
    "    modelXTrain, modelYTrain, modelXLabel, modelYLabel = train_test_split(initial80, initial80Label, test_size=0.5, shuffle = True)\n",
    "    \n",
    "    # Prepare data for light gradient boosting classification\n",
    "    Xtrain = lgb.Dataset(modelXTrain, label=modelXLabel)\n",
    "    Ytrain = lgb.Dataset(modelYTrain, label=modelYLabel)\n",
    "    \n",
    "    # Train the lgb classifier\n",
    "    clfX = lgb.train(params, Xtrain, 1000)\n",
    "    clfY = lgb.train(params, Ytrain, 1000)\n",
    "    \n",
    "    # Produce a list of probabilities\n",
    "    predictedX = clfX.predict(cifar_hog_train)\n",
    "    predictedY = clfY.predict(cifar_hog_train)\n",
    "    \n",
    "    # Convert the probabilities to a list of labels\n",
    "    XLabel = probConvert(predictedX)\n",
    "    YLabel = probConvert(predictedY)\n",
    "    \n",
    "    # Print the accuracy for each classifier on the test set\n",
    "    testPredictX = clfX.predict(cifar_hog_test)\n",
    "    testXLabel = probConvert(testPredictX)\n",
    "    testPredictY = clfY.predict(cifar_hog_test)\n",
    "    textYLabel = probConvert(testPredictY)\n",
    "    testAccX = accuracy_score(testPredictX, cifar_Yts)\n",
    "    testAccY = accuracy_score(testPredictY, cifar_Yts)\n",
    "    clfX_accuracy_average.append(testAccX)\n",
    "    clfY_accuracy_average.append(testAccY)\n",
    "    \n",
    "    # Assign the labels\n",
    "    cifar_Str = assignLabel(XLabel, YLabel, cifar_Str)\n",
    "    print('Epoch', i+1, 'Classifier X Accuracy: ', np.round(testAccX*100,2))\n",
    "    print('Epoch', i+1, 'Classifier Y Accuracy: ', np.round(testAccY*100,2))  \n",
    "    print()\n",
    "    \n",
    "    if i == 9:\n",
    "        print('Final Classifier X Average Accuracy: ', np.mean(clfX_accuracy_average))\n",
    "        print('Final Classifier X Standard Deviation', np.std(clfX_accuracy_average))\n",
    "        print('Final Classifier Y Average Accuracy: ', np.mean(clfY_accuracy_average))\n",
    "        print('Final Classifier Y Standard Deviation', np.std(clfY_accuracy_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Importance Reweighting LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Classifier Accuracy Before IR:  0.864\n",
      "Epoch 1 Classifier Accuracy After IR:  0.9235\n",
      "\n",
      "Epoch 2 Classifier Accuracy Before IR:  0.858\n",
      "Epoch 2 Classifier Accuracy After IR:  0.9275\n",
      "\n",
      "Epoch 3 Classifier Accuracy Before IR:  0.8575\n",
      "Epoch 3 Classifier Accuracy After IR:  0.917\n",
      "\n",
      "Epoch 4 Classifier Accuracy Before IR:  0.859\n",
      "Epoch 4 Classifier Accuracy After IR:  0.9175\n",
      "\n",
      "Epoch 5 Classifier Accuracy Before IR:  0.846\n",
      "Epoch 5 Classifier Accuracy After IR:  0.9145\n",
      "\n",
      "Epoch 6 Classifier Accuracy Before IR:  0.866\n",
      "Epoch 6 Classifier Accuracy After IR:  0.927\n",
      "\n",
      "Epoch 7 Classifier Accuracy Before IR:  0.856\n",
      "Epoch 7 Classifier Accuracy After IR:  0.9205\n",
      "\n",
      "Epoch 8 Classifier Accuracy Before IR:  0.8705\n",
      "Epoch 8 Classifier Accuracy After IR:  0.91\n",
      "\n",
      "Epoch 9 Classifier Accuracy Before IR:  0.861\n",
      "Epoch 9 Classifier Accuracy After IR:  0.9225\n",
      "\n",
      "Epoch 10 Classifier Accuracy Before IR:  0.864\n",
      "Epoch 10 Classifier Accuracy After IR:  0.93\n",
      "\n",
      "Before IR Average Accuracy 0.8602000000000001\n",
      "Before IR Standard Deviation 0.006341135544995088\n",
      "After IR Average Accuracy 0.9209999999999999\n",
      "After IR Average Accuracy 0.005995831885568512\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "rho1 = 0.2\n",
    "rho2 = 0.4\n",
    "initial_accuracy_average = []\n",
    "final_accuracy_average = []\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Split into two sets of training data\n",
    "    modelXTrain, modelYTrain, modelXLabel, modelYLabel = train_test_split(mnist_hog_train, mnist_Str, test_size=0.2, shuffle = True)\n",
    "\n",
    "    # Prepare data for light gradient boosting classification\n",
    "    XTrain = lgb.Dataset(modelXTrain, label=modelXLabel)\n",
    "\n",
    "    # Train the lgb classifier\n",
    "    clfX = lgb.train(params, XTrain, 1000)\n",
    "\n",
    "    # Produce a list of probabilities: one on the training set the other on the test\n",
    "    predictedX = clfX.predict(mnist_hog_train)\n",
    "    predictedY = clfX.predict(mnist_hog_test)\n",
    "\n",
    "    # Convert the probabilities to a list of labels\n",
    "    YLabel = probConvert(predictedY)\n",
    "\n",
    "    # Calculate the accuracy on test\n",
    "    initial_accuracy = accuracy_score(YLabel, mnist_Yts)\n",
    "    initial_accuracy_average.append(initial_accuracy)\n",
    "    print('Epoch',i+1,'Classifier Accuracy Before IR: ',initial_accuracy)\n",
    "\n",
    "    # Create an array to feed in for estimateBeta\n",
    "    probX = np.expand_dims(predictedX, axis=1)\n",
    "    inverseX = np.expand_dims(1-predictedX, axis=1)\n",
    "    totalprobX = np.concatenate((inverseX, probX), axis=1)\n",
    "\n",
    "    # Obtain weights for importance reweighting\n",
    "    weights = estimateBeta(modelXLabel, totalprobX, rho1, rho2)\n",
    "\n",
    "    # Pass the weights through a non-linear activation function\n",
    "    for j in range(len(weights)):\n",
    "        if weights[j] < 0:\n",
    "            weights[j] = 0.0\n",
    "\n",
    "    # Normalize the weights           \n",
    "    weights = normalise(weights)\n",
    "\n",
    "    # Remove a dimension from the weight array\n",
    "    weights = weights[:,0]\n",
    "\n",
    "    # Retrain the model with the new weights\n",
    "    newTrain = lgb.Dataset(modelXTrain, label=modelXLabel, weight=weights)\n",
    "    clfZ = lgb.train(params, newTrain, 1000)\n",
    "    predictedZ = clfZ.predict(mnist_hog_test)\n",
    "    Zlabel = probConvert(predictedZ)\n",
    "\n",
    "    # Print the final accuracy in each epoch\n",
    "    final_accuracy = accuracy_score(Zlabel, mnist_Yts)\n",
    "    final_accuracy_average.append(final_accuracy)\n",
    "    print('Epoch',i+1,'Classifier Accuracy After IR: ',final_accuracy)\n",
    "    print()\n",
    "    \n",
    "    # Print the average scores\n",
    "    if i == 9:\n",
    "        print('Before IR Average Accuracy', np.mean(initial_accuracy_average))\n",
    "        print('Before IR Standard Deviation', np.std(initial_accuracy_average))\n",
    "        print('After IR Average Accuracy', np.mean(final_accuracy_average))\n",
    "        print('After IR Average Accuracy', np.std(final_accuracy_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR Importance Reweighting LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Classifier Accuracy Before IR:  0.6835\n",
      "Epoch 1 Classifier Accuracy After IR:  0.823\n",
      "\n",
      "Epoch 2 Classifier Accuracy Before IR:  0.6775\n",
      "Epoch 2 Classifier Accuracy After IR:  0.8235\n",
      "\n",
      "Epoch 3 Classifier Accuracy Before IR:  0.663\n",
      "Epoch 3 Classifier Accuracy After IR:  0.8115\n",
      "\n",
      "Epoch 4 Classifier Accuracy Before IR:  0.665\n",
      "Epoch 4 Classifier Accuracy After IR:  0.8235\n",
      "\n",
      "Epoch 5 Classifier Accuracy Before IR:  0.671\n",
      "Epoch 5 Classifier Accuracy After IR:  0.8195\n",
      "\n",
      "Epoch 6 Classifier Accuracy Before IR:  0.683\n",
      "Epoch 6 Classifier Accuracy After IR:  0.83\n",
      "\n",
      "Epoch 7 Classifier Accuracy Before IR:  0.691\n",
      "Epoch 7 Classifier Accuracy After IR:  0.8275\n",
      "\n",
      "Epoch 8 Classifier Accuracy Before IR:  0.674\n",
      "Epoch 8 Classifier Accuracy After IR:  0.8345\n",
      "\n",
      "Epoch 9 Classifier Accuracy Before IR:  0.679\n",
      "Epoch 9 Classifier Accuracy After IR:  0.836\n",
      "\n",
      "Epoch 10 Classifier Accuracy Before IR:  0.69\n",
      "Epoch 10 Classifier Accuracy After IR:  0.8325\n",
      "\n",
      "Before IR Average Accuracy 0.6777000000000001\n",
      "Before IR Standard Deviation 0.0090972523324353\n",
      "After IR Average Accuracy 0.8261499999999999\n",
      "After IR Standard Deviation 0.007117056976025974\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs\n",
    "epochs = 10\n",
    "rho1 = 0.2\n",
    "rho2 = 0.4\n",
    "initial_accuracy_average = []\n",
    "final_accuracy_average = []\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Split into two sets of training data\n",
    "    modelXTrain, modelYTrain, modelXLabel, modelYLabel = train_test_split(cifar_hog_train, cifar_Str, test_size=0.2, shuffle = True)\n",
    "\n",
    "    # Prepare data for light gradient boosting classification\n",
    "    XTrain = lgb.Dataset(modelXTrain, label=modelXLabel)\n",
    "\n",
    "    # Train the lgb classifier\n",
    "    clfX = lgb.train(params, XTrain, 1000)\n",
    "\n",
    "    # Produce a list of probabilities: one on the training set the other on the test\n",
    "    predictedX = clfX.predict(cifar_hog_train)\n",
    "    predictedY = clfX.predict(cifar_hog_test)\n",
    "\n",
    "    # Convert the probabilities to a list of labels\n",
    "    YLabel = probConvert(predictedY)\n",
    "\n",
    "    # Calculate the accuracy on test\n",
    "    initial_accuracy = accuracy_score(YLabel, cifar_Yts)\n",
    "    initial_accuracy_average.append(initial_accuracy)\n",
    "    print('Epoch',i+1,'Classifier Accuracy Before IR: ',initial_accuracy)\n",
    "\n",
    "    # Create an array to feed in for estimateBeta\n",
    "    probX = np.expand_dims(predictedX, axis=1)\n",
    "    inverseX = np.expand_dims(1-predictedX, axis=1)\n",
    "    totalprobX = np.concatenate((inverseX, probX), axis=1)\n",
    "\n",
    "    # Obtain weights for importance reweighting\n",
    "    weights = estimateBeta(modelXLabel, totalprobX, rho1, rho2)\n",
    "\n",
    "    # Pass the weights through a non-linear activation function\n",
    "    for j in range(len(weights)):\n",
    "        if weights[j] < 0:\n",
    "            weights[j] = 0.0\n",
    "\n",
    "    # Normalize the weights           \n",
    "    weights = normalise(weights)\n",
    "\n",
    "    # Remove a dimension from the weight array\n",
    "    weights = weights[:,0]\n",
    "\n",
    "    # Retrain the model with the new weights\n",
    "    newTrain = lgb.Dataset(modelXTrain, label=modelXLabel, weight=weights)\n",
    "    clfZ = lgb.train(params, newTrain, 1000)\n",
    "    predictedZ = clfZ.predict(cifar_hog_test)\n",
    "    Zlabel = probConvert(predictedZ)\n",
    "\n",
    "    # Print the final accuracy in each epoch\n",
    "    final_accuracy = accuracy_score(Zlabel, cifar_Yts)\n",
    "    final_accuracy_average.append(final_accuracy)\n",
    "    print('Epoch',i+1,'Classifier Accuracy After IR: ',final_accuracy)\n",
    "    print()\n",
    "    \n",
    "    # Print the average scores\n",
    "    if i == 9:\n",
    "        print('Before IR Average Accuracy', np.mean(initial_accuracy_average))\n",
    "        print('Before IR Standard Deviation', np.std(initial_accuracy_average))\n",
    "        print('After IR Average Accuracy', np.mean(final_accuracy_average))\n",
    "        print('After IR Standard Deviation', np.std(final_accuracy_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Iterative Cross Learning CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 5s 618us/step - loss: 0.6236 - acc: 0.6542 - val_loss: 0.6146 - val_acc: 0.6780\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 398us/step - loss: 0.6068 - acc: 0.6775 - val_loss: 0.6025 - val_acc: 0.6790\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 359us/step - loss: 0.6017 - acc: 0.6803 - val_loss: 0.5944 - val_acc: 0.6775\n",
      "\n",
      "Initial Classifier Accuracy 90.7\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 785us/step - loss: 0.6547 - acc: 0.6205 - val_loss: 0.6104 - val_acc: 0.6750\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 411us/step - loss: 0.6154 - acc: 0.6660 - val_loss: 0.6059 - val_acc: 0.6815\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 396us/step - loss: 0.6137 - acc: 0.6712 - val_loss: 0.6023 - val_acc: 0.6760\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 773us/step - loss: 0.6193 - acc: 0.6482 - val_loss: 0.6196 - val_acc: 0.6655\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 422us/step - loss: 0.5991 - acc: 0.6823 - val_loss: 0.6016 - val_acc: 0.6610\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 471us/step - loss: 0.5907 - acc: 0.6913 - val_loss: 0.6218 - val_acc: 0.6675\n",
      "\n",
      "Epoch 1 Classifier X Accuracy:  91.5\n",
      "Epoch 1 Classifier Y Accuracy:  93.15\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 376us/step - loss: 0.6291 - acc: 0.6478 - val_loss: 0.6379 - val_acc: 0.6320\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 390us/step - loss: 0.6246 - acc: 0.6573 - val_loss: 0.6348 - val_acc: 0.6490\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 382us/step - loss: 0.6166 - acc: 0.6570 - val_loss: 0.6483 - val_acc: 0.6255\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 451us/step - loss: 0.6507 - acc: 0.6232 - val_loss: 0.6370 - val_acc: 0.6515\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 371us/step - loss: 0.6392 - acc: 0.6350 - val_loss: 0.6390 - val_acc: 0.6505\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 1s 343us/step - loss: 0.6265 - acc: 0.6605 - val_loss: 0.6433 - val_acc: 0.6245\n",
      "\n",
      "Epoch 2 Classifier X Accuracy:  91.25\n",
      "Epoch 2 Classifier Y Accuracy:  65.9\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 1s 342us/step - loss: 0.6299 - acc: 0.6350 - val_loss: 0.6045 - val_acc: 0.6705\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 365us/step - loss: 0.6213 - acc: 0.6515 - val_loss: 0.6112 - val_acc: 0.6675\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 400us/step - loss: 0.6145 - acc: 0.6552 - val_loss: 0.6085 - val_acc: 0.6580\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 377us/step - loss: 0.6239 - acc: 0.6490 - val_loss: 0.6089 - val_acc: 0.6580\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 341us/step - loss: 0.6081 - acc: 0.6640 - val_loss: 0.6088 - val_acc: 0.6740\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 1s 344us/step - loss: 0.5989 - acc: 0.6880 - val_loss: 0.6186 - val_acc: 0.6635\n",
      "\n",
      "Epoch 3 Classifier X Accuracy:  73.05\n",
      "Epoch 3 Classifier Y Accuracy:  88.65\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 1s 352us/step - loss: 0.6115 - acc: 0.6672 - val_loss: 0.6188 - val_acc: 0.6670\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 376us/step - loss: 0.6030 - acc: 0.6788 - val_loss: 0.6226 - val_acc: 0.6615\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 1s 354us/step - loss: 0.5870 - acc: 0.6928 - val_loss: 0.6282 - val_acc: 0.6660\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 1s 368us/step - loss: 0.6206 - acc: 0.6528 - val_loss: 0.6216 - val_acc: 0.6500\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 341us/step - loss: 0.6018 - acc: 0.6782 - val_loss: 0.6133 - val_acc: 0.6690\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 1s 342us/step - loss: 0.5846 - acc: 0.6990 - val_loss: 0.6266 - val_acc: 0.6520\n",
      "\n",
      "Epoch 4 Classifier X Accuracy:  82.45\n",
      "Epoch 4 Classifier Y Accuracy:  73.65\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 1s 370us/step - loss: 0.6156 - acc: 0.6623 - val_loss: 0.6022 - val_acc: 0.6690\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 343us/step - loss: 0.5947 - acc: 0.6863 - val_loss: 0.6016 - val_acc: 0.6630\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 418us/step - loss: 0.5798 - acc: 0.6990 - val_loss: 0.6087 - val_acc: 0.6650\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 464us/step - loss: 0.5932 - acc: 0.6825 - val_loss: 0.6015 - val_acc: 0.6715\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 340us/step - loss: 0.5702 - acc: 0.7098 - val_loss: 0.5999 - val_acc: 0.6765\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 1s 340us/step - loss: 0.5501 - acc: 0.7230 - val_loss: 0.6134 - val_acc: 0.6735\n",
      "\n",
      "Epoch 5 Classifier X Accuracy:  69.8\n",
      "Epoch 5 Classifier Y Accuracy:  82.15\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 1s 342us/step - loss: 0.5899 - acc: 0.6960 - val_loss: 0.6089 - val_acc: 0.6815\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 350us/step - loss: 0.5669 - acc: 0.7068 - val_loss: 0.6043 - val_acc: 0.6730\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 1s 364us/step - loss: 0.5412 - acc: 0.7263 - val_loss: 0.6137 - val_acc: 0.6755\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 1s 367us/step - loss: 0.5925 - acc: 0.6833 - val_loss: 0.5907 - val_acc: 0.6885\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 373us/step - loss: 0.5618 - acc: 0.7152 - val_loss: 0.5882 - val_acc: 0.6900\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 1s 349us/step - loss: 0.5337 - acc: 0.7382 - val_loss: 0.5833 - val_acc: 0.6790\n",
      "\n",
      "Epoch 6 Classifier X Accuracy:  77.1\n",
      "Epoch 6 Classifier Y Accuracy:  76.0\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 1s 359us/step - loss: 0.5781 - acc: 0.7010 - val_loss: 0.6224 - val_acc: 0.6820\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 1s 348us/step - loss: 0.5463 - acc: 0.7193 - val_loss: 0.6031 - val_acc: 0.6750\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 412us/step - loss: 0.5168 - acc: 0.7485 - val_loss: 0.6258 - val_acc: 0.6700\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 426us/step - loss: 0.5652 - acc: 0.7060 - val_loss: 0.5800 - val_acc: 0.6965\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 411us/step - loss: 0.5165 - acc: 0.7462 - val_loss: 0.5963 - val_acc: 0.6770\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 428us/step - loss: 0.4896 - acc: 0.7635 - val_loss: 0.6042 - val_acc: 0.6835\n",
      "\n",
      "Epoch 7 Classifier X Accuracy:  77.0\n",
      "Epoch 7 Classifier Y Accuracy:  75.05\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 460us/step - loss: 0.5686 - acc: 0.7098 - val_loss: 0.5738 - val_acc: 0.6950\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 406us/step - loss: 0.5233 - acc: 0.7385 - val_loss: 0.5829 - val_acc: 0.6915\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 462us/step - loss: 0.4906 - acc: 0.7705 - val_loss: 0.5883 - val_acc: 0.6865\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 462us/step - loss: 0.5514 - acc: 0.7195 - val_loss: 0.5759 - val_acc: 0.6910\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 397us/step - loss: 0.4939 - acc: 0.7643 - val_loss: 0.5694 - val_acc: 0.7125\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 375us/step - loss: 0.4562 - acc: 0.7865 - val_loss: 0.5884 - val_acc: 0.7045\n",
      "\n",
      "Epoch 8 Classifier X Accuracy:  76.6\n",
      "Epoch 8 Classifier Y Accuracy:  71.9\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 1s 371us/step - loss: 0.5331 - acc: 0.7375 - val_loss: 0.5507 - val_acc: 0.7250\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 384us/step - loss: 0.4834 - acc: 0.7715 - val_loss: 0.5832 - val_acc: 0.7040\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 462us/step - loss: 0.4464 - acc: 0.7973 - val_loss: 0.5954 - val_acc: 0.7080\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 379us/step - loss: 0.5304 - acc: 0.7443 - val_loss: 0.5417 - val_acc: 0.7320\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 500us/step - loss: 0.4580 - acc: 0.7872 - val_loss: 0.5480 - val_acc: 0.7345\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 444us/step - loss: 0.4109 - acc: 0.8198 - val_loss: 0.5760 - val_acc: 0.7150\n",
      "\n",
      "Epoch 9 Classifier X Accuracy:  73.05\n",
      "Epoch 9 Classifier Y Accuracy:  69.2\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 470us/step - loss: 0.5299 - acc: 0.7465 - val_loss: 0.5456 - val_acc: 0.7160\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 550us/step - loss: 0.4757 - acc: 0.7795 - val_loss: 0.5556 - val_acc: 0.7235\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 511us/step - loss: 0.4357 - acc: 0.8000 - val_loss: 0.5735 - val_acc: 0.7215\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 2s 510us/step - loss: 0.5100 - acc: 0.7620 - val_loss: 0.5091 - val_acc: 0.7575\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 393us/step - loss: 0.4395 - acc: 0.7995 - val_loss: 0.5233 - val_acc: 0.7545\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 2s 523us/step - loss: 0.3906 - acc: 0.8242 - val_loss: 0.5276 - val_acc: 0.7525\n",
      "\n",
      "Epoch 10 Classifier X Accuracy:  78.75\n",
      "Epoch 10 Classifier Y Accuracy:  74.0\n",
      "\n",
      "Final Classifier X Average Accuracy:  0.79055\n",
      "Final Classifier X Standard Deviation 0.06978984524986424\n",
      "Final Classifier Y Average Accuracy:  0.7696500000000001\n",
      "Final Classifier Y Standard Deviation 0.08112584360116078\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "dataset = np.load('/Users/joshhuang/Desktop/University/Master of Data Science/COMP5328/mnist_dataset.npz') \n",
    "Xtr = dataset[\"Xtr\"] \n",
    "Str = dataset[\"Str\"] \n",
    "Xts = dataset[\"Xts\"] \n",
    "Yts = dataset[\"Yts\"] \n",
    "\n",
    "# Create initial model\n",
    "modelZ = Sequential()\n",
    "modelZ.add(InputLayer(input_shape=(28, 28, 1)))\n",
    "modelZ.add(BatchNormalization())\n",
    "modelZ.add(Conv2D(32, (2, 2), padding='same', bias_initializer=Constant(0.01), kernel_initializer='random_uniform'))\n",
    "modelZ.add(MaxPool2D(padding='same'))\n",
    "modelZ.add(Flatten())\n",
    "modelZ.add(Dense(128,activation='relu',bias_initializer=Constant(0.01), kernel_initializer='random_uniform',))\n",
    "modelZ.add(Dense(2, activation='softmax'))\n",
    "modelZ.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Create model 1\n",
    "modelX = Sequential()\n",
    "modelX.add(InputLayer(input_shape=(28, 28, 1)))\n",
    "modelX.add(BatchNormalization())\n",
    "modelX.add(Conv2D(32, (2, 2), padding='same', bias_initializer=Constant(0.01), kernel_initializer='random_uniform'))\n",
    "modelX.add(MaxPool2D(padding='same'))\n",
    "modelX.add(Flatten())\n",
    "modelX.add(Dense(128,activation='relu',bias_initializer=Constant(0.01), kernel_initializer='random_uniform',))\n",
    "modelX.add(Dense(2, activation='softmax'))\n",
    "modelX.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Create model 2\n",
    "modelY = Sequential()\n",
    "modelY.add(InputLayer(input_shape=(28, 28, 1)))\n",
    "modelY.add(BatchNormalization())\n",
    "modelY.add(Conv2D(32, (2, 2), padding='same', bias_initializer=Constant(0.01), kernel_initializer='random_uniform'))\n",
    "modelY.add(MaxPool2D(padding='same'))\n",
    "modelY.add(Flatten())\n",
    "modelY.add(Dense(128,activation='relu',bias_initializer=Constant(0.01), kernel_initializer='random_uniform',))\n",
    "modelY.add(Dense(2, activation='softmax'))\n",
    "modelY.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Sample randomly and average results 10 times\n",
    "classifier_X_results = []\n",
    "classifier_Y_results = []\n",
    "repeat_experiment = 10\n",
    "batch_size = 75\n",
    "\n",
    "for i in range(repeat_experiment):\n",
    "    \n",
    "    # Perform first initial split of 80% training data to 20% validation data\n",
    "    initial80, initial20, initial80Label, initial20Label = train_test_split(Xtr, Str, test_size = 0.2)\n",
    "    \n",
    "    # Perform the next split so that each classifier receives half of the training data\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(initial80, initial80Label, test_size = 0.5)   \n",
    "\n",
    "    # Encode class values as integers\n",
    "    y_train = to_categorical(Y_train)\n",
    "    y_val = to_categorical(Y_val)\n",
    "    y_test = to_categorical(Yts)\n",
    "    i80label = to_categorical(initial80Label)\n",
    "    i20label = to_categorical(initial20Label)\n",
    "\n",
    "    # Reshape data to fit model\n",
    "    i80train = initial80.reshape(8000,28,28,1)\n",
    "    i20train = initial20.reshape(2000,28,28,1)\n",
    "    x_train = X_train.reshape(4000,28,28,1)\n",
    "    x_val = X_val.reshape(4000,28,28,1)\n",
    "    x_test = Xts.reshape(2000,28,28,1)\n",
    "    \n",
    "    # Train comparison model\n",
    "    if i == 0:\n",
    "        modelZ.fit(i80train,i80label,epochs=3,batch_size=batch_size,validation_data=(i20train, i20label))\n",
    "        results_classesZ = modelZ.predict_classes(x_test)\n",
    "        Z_accuracy = accuracy_score(y_test[:,1].flatten(), results_classesZ)\n",
    "        print()\n",
    "        print('Initial Classifier Accuracy', np.round(Z_accuracy*100,2))\n",
    "        print()\n",
    "\n",
    "    # Train 2 classifiers, each with a different partiion of training data    \n",
    "    modelX.fit(x_train,y_train,epochs=3,batch_size=batch_size,validation_data=(i20train, i20label))\n",
    "    modelY.fit(x_val,y_val,epochs=3,batch_size=batch_size,validation_data=(i20train, i20label))\n",
    "\n",
    "    # Predict classes using the 2 classifiers\n",
    "    results_classesX = modelX.predict_classes(x_test)\n",
    "    results_classesY = modelY.predict_classes(x_test)\n",
    "    \n",
    "    # Get a list of accuracies by comparing with test data\n",
    "    X_accuracy = accuracy_score(y_test[:,1].flatten(), results_classesX)\n",
    "    Y_accuracy = accuracy_score(y_test[:,1].flatten(), results_classesY)\n",
    "    \n",
    "    # Append the list of accuracies\n",
    "    classifier_X_results.append(X_accuracy)\n",
    "    classifier_Y_results.append(Y_accuracy)\n",
    "    \n",
    "    # Reassign labels of the training data\n",
    "    Str = assignLabel(results_classesX, results_classesY, Str)\n",
    "    \n",
    "    print()\n",
    "    print('Epoch', i+1, 'Classifier X Accuracy: ', np.round(X_accuracy*100,2))\n",
    "    print('Epoch', i+1, 'Classifier Y Accuracy: ', np.round(Y_accuracy*100,2))  \n",
    "    print()\n",
    "    \n",
    "    if i == 9:\n",
    "        print('Final Classifier X Average Accuracy: ', np.mean(classifier_X_results))\n",
    "        print('Final Classifier X Standard Deviation', np.std(classifier_X_results))\n",
    "        print('Final Classifier Y Average Accuracy: ', np.mean(classifier_Y_results))\n",
    "        print('Final Classifier Y Standard Deviation', np.std(classifier_Y_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR Iterative Cross Learning CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 7s 857us/step - loss: 0.6644 - acc: 0.6095 - val_loss: 0.6340 - val_acc: 0.6425\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 632us/step - loss: 0.6311 - acc: 0.6424 - val_loss: 0.6400 - val_acc: 0.6370\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 629us/step - loss: 0.6206 - acc: 0.6550 - val_loss: 0.6236 - val_acc: 0.6530\n",
      "\n",
      "Initial Classifier Accuracy 82.95\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6712 - acc: 0.6112 - val_loss: 0.7192 - val_acc: 0.6200\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 676us/step - loss: 0.6332 - acc: 0.6377 - val_loss: 0.6319 - val_acc: 0.6455\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 677us/step - loss: 0.6248 - acc: 0.6560 - val_loss: 0.6342 - val_acc: 0.6385\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6555 - acc: 0.6177 - val_loss: 0.7089 - val_acc: 0.6245\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 679us/step - loss: 0.6220 - acc: 0.6627 - val_loss: 0.6411 - val_acc: 0.6415\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 674us/step - loss: 0.6060 - acc: 0.6798 - val_loss: 0.6373 - val_acc: 0.6535\n",
      "\n",
      "Epoch 1 Classifier X Accuracy:  71.15\n",
      "Epoch 1 Classifier Y Accuracy:  81.3\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 677us/step - loss: 0.6366 - acc: 0.6358 - val_loss: 0.6318 - val_acc: 0.6305\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 711us/step - loss: 0.6254 - acc: 0.6483 - val_loss: 0.6345 - val_acc: 0.6300\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 842us/step - loss: 0.6137 - acc: 0.6642 - val_loss: 0.6407 - val_acc: 0.6345\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 795us/step - loss: 0.6326 - acc: 0.6458 - val_loss: 0.6278 - val_acc: 0.6435\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 776us/step - loss: 0.6090 - acc: 0.6698 - val_loss: 0.6368 - val_acc: 0.6340\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 744us/step - loss: 0.5772 - acc: 0.6962 - val_loss: 0.6537 - val_acc: 0.6125\n",
      "\n",
      "Epoch 2 Classifier X Accuracy:  79.25\n",
      "Epoch 2 Classifier Y Accuracy:  67.0\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 633us/step - loss: 0.6277 - acc: 0.6367 - val_loss: 0.6176 - val_acc: 0.6430\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 668us/step - loss: 0.6061 - acc: 0.6688 - val_loss: 0.6324 - val_acc: 0.6380\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 667us/step - loss: 0.5904 - acc: 0.6888 - val_loss: 0.6541 - val_acc: 0.6410\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 668us/step - loss: 0.6116 - acc: 0.6643 - val_loss: 0.5988 - val_acc: 0.6780\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 671us/step - loss: 0.5567 - acc: 0.7138 - val_loss: 0.6113 - val_acc: 0.6720\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 675us/step - loss: 0.4902 - acc: 0.7680 - val_loss: 0.6444 - val_acc: 0.6610\n",
      "\n",
      "Epoch 3 Classifier X Accuracy:  78.3\n",
      "Epoch 3 Classifier Y Accuracy:  65.75\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 670us/step - loss: 0.6054 - acc: 0.6747 - val_loss: 0.6231 - val_acc: 0.6490\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 665us/step - loss: 0.5646 - acc: 0.7158 - val_loss: 0.6329 - val_acc: 0.6415\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 654us/step - loss: 0.5131 - acc: 0.7500 - val_loss: 0.6750 - val_acc: 0.6235\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 630us/step - loss: 0.5751 - acc: 0.7110 - val_loss: 0.6020 - val_acc: 0.6805\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 632us/step - loss: 0.4609 - acc: 0.7915 - val_loss: 0.6211 - val_acc: 0.6785\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 702us/step - loss: 0.3544 - acc: 0.8572 - val_loss: 0.6662 - val_acc: 0.6700\n",
      "\n",
      "Epoch 4 Classifier X Accuracy:  70.8\n",
      "Epoch 4 Classifier Y Accuracy:  75.5\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 641us/step - loss: 0.6023 - acc: 0.6855 - val_loss: 0.6000 - val_acc: 0.6695\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 625us/step - loss: 0.5398 - acc: 0.7407 - val_loss: 0.6086 - val_acc: 0.6870\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 629us/step - loss: 0.4578 - acc: 0.7870 - val_loss: 0.6244 - val_acc: 0.6835\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 678us/step - loss: 0.5373 - acc: 0.7463 - val_loss: 0.5425 - val_acc: 0.7330\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 706us/step - loss: 0.3795 - acc: 0.8405 - val_loss: 0.5569 - val_acc: 0.7175\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 764us/step - loss: 0.2509 - acc: 0.9113 - val_loss: 0.6263 - val_acc: 0.7195\n",
      "\n",
      "Epoch 5 Classifier X Accuracy:  70.25\n",
      "Epoch 5 Classifier Y Accuracy:  71.8\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 720us/step - loss: 0.5501 - acc: 0.7253 - val_loss: 0.5718 - val_acc: 0.7045\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 744us/step - loss: 0.4494 - acc: 0.7900 - val_loss: 0.5904 - val_acc: 0.7040\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 724us/step - loss: 0.3671 - acc: 0.8477 - val_loss: 0.6062 - val_acc: 0.6960\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 676us/step - loss: 0.4987 - acc: 0.7833 - val_loss: 0.5123 - val_acc: 0.7615\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 679us/step - loss: 0.3177 - acc: 0.8803 - val_loss: 0.4903 - val_acc: 0.7820\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 669us/step - loss: 0.1944 - acc: 0.9377 - val_loss: 0.5374 - val_acc: 0.7745\n",
      "\n",
      "Epoch 6 Classifier X Accuracy:  67.0\n",
      "Epoch 6 Classifier Y Accuracy:  63.6\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 683us/step - loss: 0.5014 - acc: 0.7675 - val_loss: 0.5228 - val_acc: 0.7575\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 692us/step - loss: 0.3709 - acc: 0.8373 - val_loss: 0.5185 - val_acc: 0.7550\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 709us/step - loss: 0.2586 - acc: 0.9123 - val_loss: 0.5635 - val_acc: 0.7390\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 705us/step - loss: 0.4519 - acc: 0.8133 - val_loss: 0.4304 - val_acc: 0.8160\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 2s 620us/step - loss: 0.2449 - acc: 0.9135 - val_loss: 0.4506 - val_acc: 0.8200\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 636us/step - loss: 0.1356 - acc: 0.9625 - val_loss: 0.4702 - val_acc: 0.8175\n",
      "\n",
      "Epoch 7 Classifier X Accuracy:  69.85\n",
      "Epoch 7 Classifier Y Accuracy:  67.5\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 659us/step - loss: 0.4857 - acc: 0.7925 - val_loss: 0.5004 - val_acc: 0.7795\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 648us/step - loss: 0.3257 - acc: 0.8675 - val_loss: 0.4833 - val_acc: 0.7890\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 642us/step - loss: 0.2188 - acc: 0.9245 - val_loss: 0.5293 - val_acc: 0.7700\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 722us/step - loss: 0.4035 - acc: 0.8468 - val_loss: 0.4131 - val_acc: 0.8475\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 694us/step - loss: 0.1938 - acc: 0.9395 - val_loss: 0.4085 - val_acc: 0.8485\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 672us/step - loss: 0.0911 - acc: 0.9810 - val_loss: 0.4367 - val_acc: 0.8490\n",
      "\n",
      "Epoch 8 Classifier X Accuracy:  64.3\n",
      "Epoch 8 Classifier Y Accuracy:  66.15\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 712us/step - loss: 0.4349 - acc: 0.8203 - val_loss: 0.4470 - val_acc: 0.8200\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 675us/step - loss: 0.2640 - acc: 0.9025 - val_loss: 0.4578 - val_acc: 0.8220\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 704us/step - loss: 0.1625 - acc: 0.9493 - val_loss: 0.4547 - val_acc: 0.8175\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 685us/step - loss: 0.3754 - acc: 0.8690 - val_loss: 0.4287 - val_acc: 0.8455\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 716us/step - loss: 0.1599 - acc: 0.9522 - val_loss: 0.4612 - val_acc: 0.8385\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 693us/step - loss: 0.0876 - acc: 0.9758 - val_loss: 0.4646 - val_acc: 0.8455\n",
      "\n",
      "Epoch 9 Classifier X Accuracy:  69.9\n",
      "Epoch 9 Classifier Y Accuracy:  71.85\n",
      "\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 734us/step - loss: 0.3821 - acc: 0.8533 - val_loss: 0.3807 - val_acc: 0.8425\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 765us/step - loss: 0.2076 - acc: 0.9213 - val_loss: 0.3688 - val_acc: 0.8530\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 632us/step - loss: 0.1225 - acc: 0.9660 - val_loss: 0.4129 - val_acc: 0.8390\n",
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "4000/4000 [==============================] - 3s 703us/step - loss: 0.3613 - acc: 0.8795 - val_loss: 0.3337 - val_acc: 0.8825\n",
      "Epoch 2/3\n",
      "4000/4000 [==============================] - 3s 641us/step - loss: 0.1537 - acc: 0.9550 - val_loss: 0.3447 - val_acc: 0.8850\n",
      "Epoch 3/3\n",
      "4000/4000 [==============================] - 3s 632us/step - loss: 0.0737 - acc: 0.9838 - val_loss: 0.3752 - val_acc: 0.8755\n",
      "\n",
      "Epoch 10 Classifier X Accuracy:  69.45\n",
      "Epoch 10 Classifier Y Accuracy:  63.85\n",
      "\n",
      "Final Classifier X Average Accuracy:  0.7102499999999999\n",
      "Final Classifier X Standard Deviation 0.04333546469117413\n",
      "Final Classifier Y Average Accuracy:  0.6942999999999999\n",
      "Final Classifier Y Standard Deviation 0.05369087445739731\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "dataset = np.load('/Users/joshhuang/Desktop/University/Master of Data Science/COMP5328/cifar_dataset.npz') \n",
    "Xtr = dataset[\"Xtr\"] \n",
    "Str = dataset[\"Str\"] \n",
    "Xts = dataset[\"Xts\"] \n",
    "Yts = dataset[\"Yts\"] \n",
    "\n",
    "# Create initial model\n",
    "modelZ = Sequential()\n",
    "modelZ.add(InputLayer(input_shape=(32, 32, 3)))\n",
    "modelZ.add(BatchNormalization())\n",
    "modelZ.add(Conv2D(32, (2, 2), padding='same', bias_initializer=Constant(0.01), kernel_initializer='random_uniform'))\n",
    "modelZ.add(MaxPool2D(padding='same'))\n",
    "modelZ.add(Flatten())\n",
    "modelZ.add(Dense(128,activation='relu',bias_initializer=Constant(0.01), kernel_initializer='random_uniform',))\n",
    "modelZ.add(Dense(2, activation='softmax'))\n",
    "modelZ.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Create model 1\n",
    "modelX = Sequential()\n",
    "modelX.add(InputLayer(input_shape=(32, 32, 3)))\n",
    "modelX.add(BatchNormalization())\n",
    "modelX.add(Conv2D(32, (2, 2), padding='same', bias_initializer=Constant(0.01), kernel_initializer='random_uniform'))\n",
    "modelX.add(MaxPool2D(padding='same'))\n",
    "modelX.add(Flatten())\n",
    "modelX.add(Dense(128,activation='relu',bias_initializer=Constant(0.01), kernel_initializer='random_uniform',))\n",
    "modelX.add(Dense(2, activation='softmax'))\n",
    "modelX.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Create model 2\n",
    "modelY = Sequential()\n",
    "modelY.add(InputLayer(input_shape=(32, 32, 3)))\n",
    "modelY.add(BatchNormalization())\n",
    "modelY.add(Conv2D(32, (2, 2), padding='same', bias_initializer=Constant(0.01), kernel_initializer='random_uniform'))\n",
    "modelY.add(MaxPool2D(padding='same'))\n",
    "modelY.add(Flatten())\n",
    "modelY.add(Dense(128,activation='relu',bias_initializer=Constant(0.01), kernel_initializer='random_uniform',))\n",
    "modelY.add(Dense(2, activation='softmax'))\n",
    "modelY.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Sample randomly and average results 10 times\n",
    "classifier_X_results = []\n",
    "classifier_Y_results = []\n",
    "repeat_experiment = 10\n",
    "batch_size = 75\n",
    "\n",
    "for i in range(repeat_experiment):\n",
    "    \n",
    "    # Perform first initial split of 80% training data to 20% validation data\n",
    "    initial80, initial20, initial80Label, initial20Label = train_test_split(Xtr, Str, test_size = 0.2)\n",
    "    \n",
    "    # Perform the next split so that each classifier receives half of the training data\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(initial80, initial80Label, test_size = 0.5)   \n",
    "\n",
    "    # Encode class values as integers\n",
    "    y_train = to_categorical(Y_train)\n",
    "    y_val = to_categorical(Y_val)\n",
    "    y_test = to_categorical(Yts)\n",
    "    i80label = to_categorical(initial80Label)\n",
    "    i20label = to_categorical(initial20Label)\n",
    "\n",
    "    # Reshape data to fit model\n",
    "    i80train = initial80.reshape(8000,32,32,3)\n",
    "    i20train = initial20.reshape(2000,32,32,3)\n",
    "    x_train = X_train.reshape(4000,32,32,3)\n",
    "    x_val = X_val.reshape(4000,32,32,3)\n",
    "    x_test = Xts.reshape(2000,32,32,3)\n",
    "    \n",
    "    # Train comparison model\n",
    "    if i == 0:\n",
    "        modelZ.fit(i80train,i80label,epochs=3,batch_size=batch_size,validation_data=(i20train, i20label))\n",
    "        results_classesZ = modelZ.predict_classes(x_test)\n",
    "        Z_accuracy = accuracy_score(y_test[:,1].flatten(), results_classesZ)\n",
    "        print()\n",
    "        print('Initial Classifier Accuracy', np.round(Z_accuracy*100,2))\n",
    "        print()\n",
    "\n",
    "    # Train 2 classifiers, each with a different partiion of training data    \n",
    "    modelX.fit(x_train,y_train,epochs=3,batch_size=batch_size,validation_data=(i20train, i20label))\n",
    "    modelY.fit(x_val,y_val,epochs=3,batch_size=batch_size,validation_data=(i20train, i20label))\n",
    "\n",
    "    # Predict classes using the 2 classifiers\n",
    "    results_classesX = modelX.predict_classes(x_test)\n",
    "    results_classesY = modelY.predict_classes(x_test)\n",
    "    \n",
    "    # Get a list of accuracies by comparing with test data\n",
    "    X_accuracy = accuracy_score(y_test[:,1].flatten(), results_classesX)\n",
    "    Y_accuracy = accuracy_score(y_test[:,1].flatten(), results_classesY)\n",
    "    \n",
    "    # Append the list of accuracies\n",
    "    classifier_X_results.append(X_accuracy)\n",
    "    classifier_Y_results.append(Y_accuracy)\n",
    "    \n",
    "    # Reassign labels of the training data\n",
    "    Str = assignLabel(results_classesX, results_classesY, Str)\n",
    "    \n",
    "    print()\n",
    "    print('Epoch', i+1, 'Classifier X Accuracy: ', np.round(X_accuracy*100,2))\n",
    "    print('Epoch', i+1, 'Classifier Y Accuracy: ', np.round(Y_accuracy*100,2))  \n",
    "    print()\n",
    "    \n",
    "    if i == 9:\n",
    "        print('Final Classifier X Average Accuracy: ', np.mean(classifier_X_results))\n",
    "        print('Final Classifier X Standard Deviation', np.std(classifier_X_results))\n",
    "        print('Final Classifier Y Average Accuracy: ', np.mean(classifier_Y_results))\n",
    "        print('Final Classifier Y Standard Deviation', np.std(classifier_Y_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Importance Reweighting CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 738us/step - loss: 0.6258 - acc: 0.6509 - val_loss: 0.6085 - val_acc: 0.6735\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 398us/step - loss: 0.6049 - acc: 0.6783 - val_loss: 0.6133 - val_acc: 0.6680\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 422us/step - loss: 0.5982 - acc: 0.6858 - val_loss: 0.6057 - val_acc: 0.6730\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 4s 470us/step - loss: 0.3092 - acc: 0.6803 - val_loss: 1.0133 - val_acc: 0.6535\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 382us/step - loss: 0.2791 - acc: 0.6850 - val_loss: 0.9780 - val_acc: 0.6685\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 386us/step - loss: 0.2712 - acc: 0.6895 - val_loss: 1.0350 - val_acc: 0.6595\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 4s 456us/step - loss: 0.6099 - acc: 0.6762 - val_loss: 0.5941 - val_acc: 0.6850\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 391us/step - loss: 0.5799 - acc: 0.7018 - val_loss: 0.5873 - val_acc: 0.6945\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 409us/step - loss: 0.5708 - acc: 0.7044 - val_loss: 0.5926 - val_acc: 0.7015\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 4s 505us/step - loss: 0.1995 - acc: 0.6984 - val_loss: 1.1202 - val_acc: 0.6970\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 366us/step - loss: 0.1626 - acc: 0.7060 - val_loss: 1.0784 - val_acc: 0.6950\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 366us/step - loss: 0.1444 - acc: 0.7165 - val_loss: 1.1934 - val_acc: 0.6940\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 431us/step - loss: 0.6025 - acc: 0.7014 - val_loss: 0.5504 - val_acc: 0.7355\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 358us/step - loss: 0.5429 - acc: 0.7339 - val_loss: 0.5559 - val_acc: 0.7250\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 363us/step - loss: 0.5249 - acc: 0.7431 - val_loss: 0.5706 - val_acc: 0.7155\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 429us/step - loss: 0.2114 - acc: 0.7425 - val_loss: 0.8321 - val_acc: 0.7300\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 360us/step - loss: 0.1723 - acc: 0.7559 - val_loss: 0.8559 - val_acc: 0.7260\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 363us/step - loss: 0.1479 - acc: 0.7707 - val_loss: 1.0058 - val_acc: 0.7280\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 434us/step - loss: 0.5256 - acc: 0.7590 - val_loss: 0.5106 - val_acc: 0.7675\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 367us/step - loss: 0.4638 - acc: 0.7885 - val_loss: 0.5186 - val_acc: 0.7610\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 368us/step - loss: 0.4376 - acc: 0.7989 - val_loss: 0.5298 - val_acc: 0.7665\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 432us/step - loss: 0.1760 - acc: 0.8070 - val_loss: 0.7545 - val_acc: 0.7580\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 367us/step - loss: 0.1337 - acc: 0.8214 - val_loss: 0.8419 - val_acc: 0.7585\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 363us/step - loss: 0.1121 - acc: 0.8326 - val_loss: 0.8488 - val_acc: 0.7440\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 429us/step - loss: 0.4535 - acc: 0.8031 - val_loss: 0.4179 - val_acc: 0.8125\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 366us/step - loss: 0.3818 - acc: 0.8321 - val_loss: 0.4535 - val_acc: 0.8020\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 369us/step - loss: 0.3584 - acc: 0.8474 - val_loss: 0.4350 - val_acc: 0.8070\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 433us/step - loss: 0.1563 - acc: 0.8544 - val_loss: 0.5238 - val_acc: 0.8110\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 374us/step - loss: 0.1206 - acc: 0.8679 - val_loss: 0.5610 - val_acc: 0.8085\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 373us/step - loss: 0.0979 - acc: 0.8809 - val_loss: 0.5872 - val_acc: 0.8105\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 435us/step - loss: 0.3668 - acc: 0.8487 - val_loss: 0.3608 - val_acc: 0.8570\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 370us/step - loss: 0.2953 - acc: 0.8781 - val_loss: 0.3608 - val_acc: 0.8590\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 360us/step - loss: 0.2547 - acc: 0.8971 - val_loss: 0.3611 - val_acc: 0.8540\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 432us/step - loss: 0.1221 - acc: 0.9080 - val_loss: 0.4317 - val_acc: 0.8510\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 366us/step - loss: 0.0959 - acc: 0.9168 - val_loss: 0.4817 - val_acc: 0.8355\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 364us/step - loss: 0.0798 - acc: 0.9230 - val_loss: 0.4765 - val_acc: 0.8590\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 433us/step - loss: 0.2716 - acc: 0.8918 - val_loss: 0.2967 - val_acc: 0.8860\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 360us/step - loss: 0.2144 - acc: 0.9161 - val_loss: 0.2653 - val_acc: 0.8925\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 365us/step - loss: 0.1848 - acc: 0.9306 - val_loss: 0.2725 - val_acc: 0.8925\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 428us/step - loss: 0.0775 - acc: 0.9389 - val_loss: 0.3027 - val_acc: 0.8900\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 362us/step - loss: 0.0579 - acc: 0.9460 - val_loss: 0.3132 - val_acc: 0.8880\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 370us/step - loss: 0.0480 - acc: 0.9503 - val_loss: 0.3365 - val_acc: 0.8850\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 431us/step - loss: 0.2216 - acc: 0.9166 - val_loss: 0.2011 - val_acc: 0.9300\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 368us/step - loss: 0.1941 - acc: 0.9229 - val_loss: 0.2070 - val_acc: 0.9230\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 369us/step - loss: 0.1480 - acc: 0.9440 - val_loss: 0.1901 - val_acc: 0.9270\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 432us/step - loss: 0.0617 - acc: 0.9550 - val_loss: 0.1945 - val_acc: 0.9270\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 373us/step - loss: 0.0477 - acc: 0.9619 - val_loss: 0.1983 - val_acc: 0.9225\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 372us/step - loss: 0.0438 - acc: 0.9641 - val_loss: 0.2199 - val_acc: 0.9240\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 436us/step - loss: 0.1538 - acc: 0.9424 - val_loss: 0.1686 - val_acc: 0.9395\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 370us/step - loss: 0.1426 - acc: 0.9474 - val_loss: 0.2227 - val_acc: 0.9160\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 370us/step - loss: 0.1310 - acc: 0.9541 - val_loss: 0.1877 - val_acc: 0.9285\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 3s 436us/step - loss: 0.0512 - acc: 0.9655 - val_loss: 0.1654 - val_acc: 0.9335\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 373us/step - loss: 0.0434 - acc: 0.9708 - val_loss: 0.1706 - val_acc: 0.9360\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 372us/step - loss: 0.0311 - acc: 0.9755 - val_loss: 0.1802 - val_acc: 0.9320\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 3s 420us/step - loss: 0.1823 - acc: 0.9420 - val_loss: 0.1807 - val_acc: 0.9290\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 359us/step - loss: 0.1138 - acc: 0.9601 - val_loss: 0.1420 - val_acc: 0.9510\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 360us/step - loss: 0.1038 - acc: 0.9686 - val_loss: 0.1602 - val_acc: 0.9445\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 4s 450us/step - loss: 0.0399 - acc: 0.9736 - val_loss: 0.1494 - val_acc: 0.9485\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 3s 355us/step - loss: 0.0260 - acc: 0.9804 - val_loss: 0.1408 - val_acc: 0.9520\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 3s 358us/step - loss: 0.0196 - acc: 0.9828 - val_loss: 0.1446 - val_acc: 0.9505\n",
      "\n",
      "Average Accuracy No IR:  0.7798\n",
      "Average Standard Deviation No IR:  0.07421866342100214\n",
      "Average Accuracy With IR:  0.8033000000000001\n",
      "Average Standard Deviation With IR:  0.08321844747410276\n"
     ]
    }
   ],
   "source": [
    "def estimateBeta(train_labels,prob,rho0,rho1):\n",
    "    n = len(train_labels)\n",
    "    beta = np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        if train_labels[:,1][i].any()==1:\n",
    "            beta[i] = (prob[i][1]-rho0)/((1-rho0-rho1)*prob[i][1]+1e-5)\n",
    "        else:\n",
    "            beta[i] = (prob[i][0]-rho1)/((1-rho0-rho1)*(prob[i][0])+1e-5)\n",
    "    return beta\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = np.load('/Users/joshhuang/Desktop/University/Master of Data Science/COMP5328/mnist_dataset.npz') \n",
    "\n",
    "Xtr = dataset[\"Xtr\"] \n",
    "Str = dataset[\"Str\"] \n",
    "Xts = dataset[\"Xts\"] \n",
    "Yts = dataset[\"Yts\"] \n",
    "\n",
    "# Create CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add model layers\n",
    "model.add(InputLayer(input_shape=(28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (2, 2), padding='same', bias_initializer=Constant(0.01), kernel_initializer='random_uniform'))\n",
    "model.add(MaxPool2D(padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu',bias_initializer=Constant(0.01), kernel_initializer='random_uniform',))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Sample randomly and average results 10 times\n",
    "accuracy_results = []\n",
    "accuracy_IR = []\n",
    "repeat_experiment = 10\n",
    "\n",
    "for i in range(repeat_experiment):\n",
    "\n",
    "    # Partition data into 80% and 20% training to validation split\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(Xtr, Str, test_size = 0.2)\n",
    "    batch_size = 75\n",
    "\n",
    "    # Encode class values as integers\n",
    "    y_train = to_categorical(Y_train)\n",
    "    y_val = to_categorical(Y_val)\n",
    "    y_test = to_categorical(Yts)\n",
    "\n",
    "    # Reshape data to fit model\n",
    "    x_train = X_train.reshape(8000,28,28,1)\n",
    "    x_val = X_val.reshape(2000,28,28,1)\n",
    "    x_test = Xts.reshape(2000,28,28,1)\n",
    "\n",
    "    # Train initial model and produce an initial set of predictions\n",
    "    model.fit(x_train,y_train,epochs=3,batch_size=batch_size,validation_data=(x_val, y_val))\n",
    "    results_classes = model.predict_classes(x_test)\n",
    "    accuracy = accuracy_score(y_test[:,1].flatten(), results_classes)\n",
    "    accuracy_results.append(accuracy)\n",
    "    print('Epoch',i+1,'Classifier Accuracy Before IR: ', accuracy)\n",
    "    \n",
    "    # Apply importance reweighting\n",
    "    rho0 = 0.2\n",
    "    rho1 = 0.4\n",
    "    prob = model.predict(x_train)\n",
    "    weights = estimateBeta(y_train, prob, rho0, rho1)\n",
    "\n",
    "    # Apply non-linear activation function and normalize the weights\n",
    "    for j in range(len(weights)):\n",
    "        if weights[j] < 0:\n",
    "            weights[j] = 0.0\n",
    "\n",
    "    weights = normalise(weights)\n",
    "\n",
    "    # Retrain model with new IR weights produce a new set of predictions\n",
    "    model.fit(x_train,y_train,epochs=3,batch_size=batch_size,validation_data=(x_val, y_val), sample_weight = weights.flatten())    \n",
    "    results_final_importance_weighting = model.predict_classes(x_test)\n",
    "    accuracy_final_importance_weighting = accuracy_score(y_test[:,1].flatten(), results_final_importance_weighting)\n",
    "    accuracy_IR.append(accuracy_final_importance_weighting)\n",
    "    print('Epoch',i+1,'Classifier Accuracy After IR: ',accuracy_final_importance_weighting)\n",
    "\n",
    "# Prediction Results\n",
    "accuracy_without_IR = np.mean(accuracy_results)\n",
    "std_without_IR = np.std(accuracy_results)\n",
    "\n",
    "accuracy_with_IR = np.mean(accuracy_IR)\n",
    "std_with_IR = np.std(accuracy_IR)\n",
    "\n",
    "print()\n",
    "print('Average Accuracy No IR: ',accuracy_without_IR)\n",
    "print('Average Standard Deviation No IR: ',std_without_IR)\n",
    "print('Average Accuracy With IR: ',accuracy_with_IR)\n",
    "print('Average Standard Deviation With IR: ',std_with_IR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR Importance Reweighting CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 8s 947us/step - loss: 0.6546 - acc: 0.6270 - val_loss: 0.6693 - val_acc: 0.6340\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 608us/step - loss: 0.6240 - acc: 0.6536 - val_loss: 0.6335 - val_acc: 0.6490\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 607us/step - loss: 0.6122 - acc: 0.6686 - val_loss: 0.6301 - val_acc: 0.6410\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 5s 669us/step - loss: 0.4510 - acc: 0.6619 - val_loss: 0.7447 - val_acc: 0.6510\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 606us/step - loss: 0.4065 - acc: 0.6890 - val_loss: 0.8488 - val_acc: 0.6240\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 609us/step - loss: 0.3709 - acc: 0.7141 - val_loss: 0.8389 - val_acc: 0.6375\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 697us/step - loss: 0.5621 - acc: 0.7236 - val_loss: 0.5774 - val_acc: 0.7085\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 626us/step - loss: 0.4950 - acc: 0.7705 - val_loss: 0.5897 - val_acc: 0.7020\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 635us/step - loss: 0.4203 - acc: 0.8144 - val_loss: 0.6389 - val_acc: 0.6950\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 708us/step - loss: 0.2089 - acc: 0.8465 - val_loss: 0.8532 - val_acc: 0.6810\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 632us/step - loss: 0.1431 - acc: 0.8810 - val_loss: 0.9913 - val_acc: 0.6665\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 630us/step - loss: 0.0742 - acc: 0.9188 - val_loss: 1.0963 - val_acc: 0.6780\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 692us/step - loss: 0.4165 - acc: 0.8328 - val_loss: 0.4763 - val_acc: 0.8035\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 628us/step - loss: 0.2279 - acc: 0.9164 - val_loss: 0.4013 - val_acc: 0.8370\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 625us/step - loss: 0.1382 - acc: 0.9561 - val_loss: 0.4477 - val_acc: 0.8255\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 698us/step - loss: 0.0581 - acc: 0.9790 - val_loss: 0.4822 - val_acc: 0.8390\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 635us/step - loss: 0.0255 - acc: 0.9901 - val_loss: 0.4968 - val_acc: 0.8400\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 635us/step - loss: 0.0139 - acc: 0.9931 - val_loss: 0.5320 - val_acc: 0.8485\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 697us/step - loss: 0.2165 - acc: 0.9298 - val_loss: 0.2093 - val_acc: 0.9225\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 630us/step - loss: 0.1110 - acc: 0.9626 - val_loss: 0.1866 - val_acc: 0.9360\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 636us/step - loss: 0.0520 - acc: 0.9839 - val_loss: 0.1716 - val_acc: 0.9360\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 705us/step - loss: 0.0167 - acc: 0.9950 - val_loss: 0.1674 - val_acc: 0.9465\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 636us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.1501 - val_acc: 0.9565\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 641us/step - loss: 0.0054 - acc: 0.9973 - val_loss: 0.1551 - val_acc: 0.9505\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 703us/step - loss: 0.0903 - acc: 0.9698 - val_loss: 0.1422 - val_acc: 0.9505\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 637us/step - loss: 0.0532 - acc: 0.9825 - val_loss: 0.1015 - val_acc: 0.9680\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 639us/step - loss: 0.0343 - acc: 0.9900 - val_loss: 0.0831 - val_acc: 0.9715\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 701us/step - loss: 0.0090 - acc: 0.9975 - val_loss: 0.0634 - val_acc: 0.9795\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 638us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0602 - val_acc: 0.9800\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 649us/step - loss: 0.0043 - acc: 0.9981 - val_loss: 0.0602 - val_acc: 0.9800\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 712us/step - loss: 0.0459 - acc: 0.9879 - val_loss: 0.1216 - val_acc: 0.9560\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 665us/step - loss: 0.0890 - acc: 0.9724 - val_loss: 0.1445 - val_acc: 0.9485\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 639us/step - loss: 0.0541 - acc: 0.9838 - val_loss: 0.0690 - val_acc: 0.9750\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 781us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0525 - val_acc: 0.9820\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 6s 801us/step - loss: 0.0081 - acc: 0.9965 - val_loss: 0.0377 - val_acc: 0.9850\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 6s 774us/step - loss: 0.0038 - acc: 0.9985 - val_loss: 0.0344 - val_acc: 0.9855\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 7s 861us/step - loss: 0.0606 - acc: 0.9821 - val_loss: 0.0875 - val_acc: 0.9740\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 657us/step - loss: 0.0632 - acc: 0.9799 - val_loss: 0.1390 - val_acc: 0.9550\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 644us/step - loss: 0.0458 - acc: 0.9865 - val_loss: 0.0831 - val_acc: 0.9710\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 7s 822us/step - loss: 0.0246 - acc: 0.9895 - val_loss: 0.0598 - val_acc: 0.9760\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 670us/step - loss: 0.0112 - acc: 0.9955 - val_loss: 0.0507 - val_acc: 0.9835\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 635us/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0421 - val_acc: 0.9860\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 705us/step - loss: 0.0450 - acc: 0.9868 - val_loss: 0.0309 - val_acc: 0.9885\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 641us/step - loss: 0.0420 - acc: 0.9875 - val_loss: 0.0488 - val_acc: 0.9855\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 628us/step - loss: 0.0183 - acc: 0.9946 - val_loss: 0.0470 - val_acc: 0.9860\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 700us/step - loss: 0.0088 - acc: 0.9961 - val_loss: 0.0444 - val_acc: 0.9850\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 636us/step - loss: 0.0164 - acc: 0.9948 - val_loss: 0.0292 - val_acc: 0.9895\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 630us/step - loss: 0.0071 - acc: 0.9984 - val_loss: 0.0267 - val_acc: 0.9890\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 699us/step - loss: 0.0208 - acc: 0.9943 - val_loss: 0.0149 - val_acc: 0.9950\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 637us/step - loss: 0.0188 - acc: 0.9950 - val_loss: 0.0146 - val_acc: 0.9945\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 638us/step - loss: 0.0106 - acc: 0.9981 - val_loss: 0.0111 - val_acc: 0.9970\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 709us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0100 - val_acc: 0.9970\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 6s 729us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0068 - val_acc: 0.9985\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 638us/step - loss: 7.9299e-04 - acc: 0.9998 - val_loss: 0.0079 - val_acc: 0.9975\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 7s 864us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0109 - val_acc: 0.9975\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 677us/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0038 - val_acc: 0.9990\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 6s 742us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0024 - val_acc: 0.9995\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 6s 728us/step - loss: 9.2891e-04 - acc: 0.9998 - val_loss: 0.0026 - val_acc: 0.9990\n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s 643us/step - loss: 4.6112e-04 - acc: 0.9998 - val_loss: 0.0038 - val_acc: 0.9985\n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s 646us/step - loss: 2.9330e-04 - acc: 0.9999 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "\n",
      "Average Accuracy No IR:  0.7222\n",
      "Average Standard Deviation No IR:  0.02620038167660922\n",
      "Average Accuracy With IR:  0.7356\n",
      "Average Standard Deviation With IR:  0.04546141220859731\n"
     ]
    }
   ],
   "source": [
    "def estimateBeta(train_labels,prob,rho0,rho1):\n",
    "    n = len(train_labels)\n",
    "    beta = np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        if train_labels[:,1][i].any()==1:\n",
    "            beta[i] = (prob[i][1]-rho0)/((1-rho0-rho1)*prob[i][1]+1e-5)\n",
    "        else:\n",
    "            beta[i] = (prob[i][0]-rho1)/((1-rho0-rho1)*(prob[i][0])+1e-5)\n",
    "    return beta\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = np.load('/Users/joshhuang/Desktop/University/Master of Data Science/COMP5328/cifar_dataset.npz') \n",
    "Xtr = dataset[\"Xtr\"] \n",
    "Str = dataset[\"Str\"] \n",
    "Xts = dataset[\"Xts\"] \n",
    "Yts = dataset[\"Yts\"] \n",
    "\n",
    "# Create CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add model layers\n",
    "model.add(InputLayer(input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (2, 2), padding='same', bias_initializer=Constant(0.01), kernel_initializer='random_uniform'))\n",
    "model.add(MaxPool2D(padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu',bias_initializer=Constant(0.01), kernel_initializer='random_uniform',))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Sample randomly and average results 10 times\n",
    "accuracy_results = []\n",
    "accuracy_IR = []\n",
    "repeat_experiment = 10\n",
    "\n",
    "for i in range(repeat_experiment):\n",
    "\n",
    "    # Partition data into 80% and 20% training to validation split\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(Xtr, Str, test_size = 0.2)\n",
    "    batch_size = 75\n",
    "\n",
    "    # Encode class values as integers\n",
    "    y_train = to_categorical(Y_train)\n",
    "    y_val = to_categorical(Y_val)\n",
    "    y_test = to_categorical(Yts)\n",
    "\n",
    "    # Reshape data to fit model\n",
    "    x_train = X_train.reshape(8000,32,32,3)\n",
    "    x_val = X_val.reshape(2000,32,32,3)\n",
    "    x_test = Xts.reshape(2000,32,32,3)\n",
    "\n",
    "    # Train initial model and produce initial set of predictions\n",
    "    model.fit(x_train,y_train,epochs=3,batch_size=batch_size,validation_data=(x_val, y_val))\n",
    "    results_classes = model.predict_classes(x_test)\n",
    "    accuracy = accuracy_score(y_test[:,1].flatten(), results_classes)\n",
    "    accuracy_results.append(accuracy)\n",
    "    print('Epoch',i+1,'Classifier Accuracy Before IR: ', accuracy)\n",
    "    \n",
    "    # Apply importance reweighting\n",
    "    rho0 = 0.2\n",
    "    rho1 = 0.4\n",
    "    prob = model.predict(x_train)\n",
    "    weights = estimateBeta(y_train, prob, rho0, rho1)\n",
    "\n",
    "    # Apply non-linear activation function and normalize the weights\n",
    "    for j in range(len(weights)):\n",
    "        if weights[j] < 0:\n",
    "            weights[j] = 0.0\n",
    "        \n",
    "    weights = normalise(weights)\n",
    "    \n",
    "    # Retrain model with new IR weights and create a new set of predictions\n",
    "    model.fit(x_train,y_train,epochs=3,batch_size=batch_size,validation_data=(x_val, y_val), sample_weight = weights.flatten())    \n",
    "    results_final_importance_weighting = model.predict_classes(x_test)\n",
    "    accuracy_final_importance_weighting = accuracy_score(y_test[:,1].flatten(), results_final_importance_weighting)\n",
    "    accuracy_IR.append(accuracy_final_importance_weighting)\n",
    "    print('Epoch',i+1,'Classifier Accuracy After IR: ',accuracy_final_importance_weighting)\n",
    "\n",
    "# Prediction results    \n",
    "accuracy_without_IR = np.mean(accuracy_results)\n",
    "std_without_IR = np.std(accuracy_results)\n",
    "\n",
    "accuracy_with_IR = np.mean(accuracy_IR)\n",
    "std_with_IR = np.std(accuracy_IR)\n",
    "\n",
    "print()\n",
    "print('Average Accuracy No IR: ',accuracy_without_IR)\n",
    "print('Average Standard Deviation No IR: ',std_without_IR)\n",
    "print('Average Accuracy With IR: ',accuracy_with_IR)\n",
    "print('Average Standard Deviation With IR: ',std_with_IR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Rate Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was our attempt to estimate the noise rate using the paper. Although unsucessful, we have outlined our method here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(pd.DataFrame(prob*100).iloc[:,0])\n",
    "counts = pd.DataFrame(y_train).iloc[:,1].value_counts()\n",
    "counts[0]/(counts[0]+counts[1])\n",
    "counts[1]/(counts[0]+counts[1])\n",
    "\n",
    "left = pd.DataFrame(prob)\n",
    "right_ytrain = pd.DataFrame(y_train).iloc[:,1]\n",
    "right_predicted = pd.DataFrame(model.predict_classes(x_train))\n",
    "table = pd.concat((left, right_predicted), axis = 1)\n",
    "table.columns = ('0','1', 'labels')\n",
    "\n",
    "ro0 = table[table.iloc[:,2]==0].iloc[:,1].min()\n",
    "ro1 = table[table.iloc[:,2]==1].iloc[:,0].min()\n",
    "print(ro0)\n",
    "print(ro1)\n",
    "#plt.hist(table[table.iloc[:,2]==1].iloc[:,1], bins = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
