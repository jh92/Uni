{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: (2 Leaky ReLU, 1 Softmax) Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:237: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:237: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss:3.772648\n",
      "Function Runtime: 0.2 minutes\n",
      "Accuracy over test data for model 1 is 0.5584444444444444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.54      0.66      1815\n",
      "          1       0.55      0.97      0.70      1823\n",
      "          2       0.35      0.71      0.47      1879\n",
      "          3       0.56      0.36      0.44      1798\n",
      "          4       0.45      0.22      0.30      1821\n",
      "          5       0.84      0.39      0.54      1782\n",
      "          6       0.35      0.22      0.27      1747\n",
      "          7       0.77      0.85      0.81      1756\n",
      "          8       0.72      0.37      0.49      1769\n",
      "          9       0.56      0.92      0.70      1810\n",
      "\n",
      "avg / total       0.60      0.56      0.54     18000\n",
      "\n",
      "Predictions  0.0   1.0   2.0  3.0  4.0  5.0  6.0   7.0  8.0   9.0\n",
      "Targets                                                          \n",
      "0            973   172   106  221   20    2  283     1   36     1\n",
      "1              0  1776    18   12    1    0   11     0    4     1\n",
      "2             16   109  1338   18  187    4  148     0   17    42\n",
      "3              6   844    41  649   96    1   74     0   87     0\n",
      "4              1   150   993  109  407    1   72     1   28    59\n",
      "5              0    52    66    0    4  702    8   304   51   595\n",
      "6            134    87   744  149  148    3  385     0   27    70\n",
      "7              0     6     2    0    0   69    0  1500    4   175\n",
      "8              0    38   477    4   39   35  108    45  659   364\n",
      "9              0     4     6    2    4   19    8   104    0  1663\n"
     ]
    }
   ],
   "source": [
    "'Load the packages'\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "'Import the data'\n",
    "\n",
    "with h5py.File('train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "with h5py.File('test_128.h5','r') as H:\n",
    "    actual_test_data = np.copy(H['data'])\n",
    "    \n",
    "'Perform preprocessing' \n",
    "\n",
    "## Create a label list from the label array for prediction purposes\n",
    "label_list = []\n",
    "for i in label:\n",
    "    label_list.append(i)\n",
    "\n",
    "## Create a label matrix for computational purposes\n",
    "label_matrix = []\n",
    "for i in label:\n",
    "    vect = [0]*10\n",
    "    vect[i] = 1\n",
    "    label_matrix.append(vect)\n",
    "label_matrix = np.array(label_matrix)\n",
    "\n",
    "## Train test split\n",
    "train_data , test_data = data[:42000,:], data[42000:,:]\n",
    "train_label, test_label = label_matrix[:42000], label_matrix[42000:]\n",
    "    \n",
    "'Activation Functions'\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, x):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - x**2\n",
    "    \n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, x):\n",
    "        return  x * (1 - x)\n",
    "    \n",
    "    def __relu(self, x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def __relu_deriv(self, x):\n",
    "        return 1 * (x > 0)\n",
    "    \n",
    "    def __leaky_relu(self, x, alpha=0.01):\n",
    "        return alpha * -1 * x * (x < 0) + x * (x > 0)\n",
    "    \n",
    "    def __leaky_relu_deriv(self, x, alpha=0.01):\n",
    "        return alpha * (x < 0) + 1 * (x > 0)\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        e = np.exp(x-np.max(x))  # Stable softmax\n",
    "        if e.ndim == 1:\n",
    "            return e / np.sum(e, axis=0) # Number of dimensions 1\n",
    "        else:  \n",
    "            return e / np.array([np.sum(e, axis=1)]).T \n",
    "    \n",
    "    def __softmax_deriv(self,x):\n",
    "        e = np.exp(x-np.max(x)) \n",
    "        return e/np.sum(e) - (e/np.sum(e))**2\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.f = self.__leaky_relu\n",
    "            self.f_deriv = self.__leaky_relu_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv  \n",
    "\n",
    "'Hidden Layer Class'\n",
    "\n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out, W=None, b=None, rng = None,\n",
    "                 activation='tanh'):\n",
    "\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.activation_deriv=Activation(activation).f_deriv\n",
    "        \n",
    "        ## Initialize attentuation constant for momentum\n",
    "        global gamma\n",
    "        gamma = 0.9\n",
    "        \n",
    "        ## Rng check for dropout\n",
    "        if rng is None:\n",
    "            self.rng = np.random.RandomState(1234)\n",
    "        \n",
    "        ## Initialize weights and bias\n",
    "        self.W = np.random.normal(0, 1 ,(n_in,n_out))\n",
    "        self.b = np.zeros(n_out,)       \n",
    "        \n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "    \n",
    "        # Initialize gradients of W, b     \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        # Initialize momentum values\n",
    "        self.momentum_W = 0\n",
    "        self.momentum_b = 0\n",
    "        self.momentum_g = 0\n",
    "        self.momentum_v = 0\n",
    "        \n",
    "        # This value checks to see that the initialization of the weight norm process occurs only once\n",
    "        self.first_pass = 0\n",
    "        \n",
    "    def forward(self, input):\n",
    " \n",
    "        # Calculate linear output of initial feedforward pass\n",
    "        lin_output = np.dot(input, self.W) + self.b   \n",
    "        \n",
    "        # If initialization hasn't occurred, do so once\n",
    "        if self.first_pass == 0:\n",
    "            \n",
    "            # Calculate standard deviation and mean \n",
    "            std_lin_output = np.std(lin_output)\n",
    "            mean_lin_output = np.mean(lin_output)\n",
    "            \n",
    "            # Calculate g, v and b for initial pass\n",
    "            self.g = 1/std_lin_output\n",
    "            self.v = np.random.normal(0, 1, self.W.shape)   \n",
    "            self.b = -mean_lin_output/std_lin_output\n",
    "            \n",
    "            self.first_pass += 1\n",
    "        \n",
    "        # Calculate the norm of v\n",
    "        v_norm = np.linalg.norm(self.v)\n",
    "        \n",
    "        # Reconstruct W\n",
    "        self.W = self.g*self.v/v_norm   \n",
    "        \n",
    "        # Calculate new linear output\n",
    "        new_lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            new_lin_output if self.activation is None\n",
    "            else self.activation(new_lin_output)\n",
    "        )    \n",
    "        \n",
    "        # Pass these values to next layer\n",
    "        self.input=input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \n",
    "        # Calculate the new gradients using momentum update\n",
    "        \n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta)) + gamma * self.momentum_W\n",
    "        self.grad_b = np.sum(delta, axis = 0) + gamma * self.momentum_b\n",
    "        v_norm = np.linalg.norm(self.v)\n",
    "        self.grad_g = self.grad_W * self.v / v_norm + gamma * self.momentum_g\n",
    "        self.grad_v = self.g * self.grad_W/v_norm - self.g*self.grad_g * self.v/(v_norm)**2 + gamma * self.momentum_v\n",
    "        delta_ = (delta.dot(self.W.T) * self.activation_deriv(self.input))\n",
    "        \n",
    "        # Return delta_ for next layer\n",
    "        return delta_\n",
    "        \n",
    "    def dropout(self, input, prob_dropout):\n",
    "\n",
    "        dropout_mask = self.rng.binomial(size=input.shape,n=1, p=1-prob_dropout)\n",
    "        return dropout_mask\n",
    "    \n",
    "'Multi-Layer Perceptron'\n",
    " \n",
    "## Create a container for dropout    \n",
    "dropout_masks = []      \n",
    "    \n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, layers,p_dropout=0.5, rng = None, activation='tanh'):\n",
    "\n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        dropout_masks = []\n",
    "        \n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState(1234)\n",
    "        \n",
    "        self.activation=activation\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            # Hidden to Output\n",
    "            if (i % len(layers) - 2) == 0:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation='softmax'))\n",
    "            # Hidden to Hidden\n",
    "            elif (i % len(layers) - 1) == 0:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation='leaky_relu'))\n",
    "            # Input to Hidden\n",
    "            else:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation='leaky_relu'))\n",
    "            \n",
    "    def forward(self,input, dropout = False, p_dropout = 0.5):\n",
    "        for i in range(len(self.layers)):\n",
    "            output=self.layers[i].forward(input)\n",
    "            input=output\n",
    "            if dropout == True and i < len(self.layers)-1:\n",
    "                mask = self.layers[i].dropout(input = output, prob_dropout = p_dropout)\n",
    "                output *= mask\n",
    "                dropout_masks.append(mask)\n",
    "                input=output\n",
    "            self.output = output\n",
    "        return output\n",
    "    \n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation).f_deriv\n",
    "        error = (y-y_hat)\n",
    "        \n",
    "        ## Cross entropy loss\n",
    "        m = 1/y.shape[0]      \n",
    "        loss = -m * np.sum(y * np.log(np.absolute(y_hat)))\n",
    "        \n",
    "        ## divide by mini batch size\n",
    "        delta= (error/30)\n",
    "        return loss,delta      \n",
    "            \n",
    "    def backward(self,delta, dropout = False, p_dropout = 0.5):        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            delta= self.layers[i].backward(delta)\n",
    "            if dropout == True:\n",
    "                if i > 0:\n",
    "                    delta *= dropout_masks[i-1]\n",
    "        del dropout_masks[:]\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            # Calculate new momentums of W, b, g and v and update each parameter\n",
    "            layer.momentum_W = layer.momentum_W * gamma + lr * layer.grad_W\n",
    "            layer.momentum_b = layer.momentum_b * gamma + lr * layer.grad_b\n",
    "            layer.momentum_g = layer.momentum_g * gamma + lr * layer.grad_g\n",
    "            layer.momentum_v = layer.momentum_v * gamma + lr * layer.grad_v\n",
    "            layer.W += layer.momentum_W\n",
    "            layer.b += layer.momentum_b\n",
    "            layer.g += layer.momentum_g\n",
    "            layer.v += layer.momentum_v\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1,dropout = False, p_dropout = 0.5, batchsize = 30, rng = None, epochs=100):\n",
    "        \n",
    "        # Mini-batch size\n",
    "        num_of_batches = train_data.shape[0]/batchsize\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            \n",
    "            shuffled_train, shuffled_label = shuffle(train_data, train_label, random_state = 1)\n",
    "            \n",
    "            bXY = zip(np.array_split(shuffled_train, num_of_batches, axis = 0),\n",
    "                      np.array_split(shuffled_label, num_of_batches, axis = 0))\n",
    "            \n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for bX, bY in bXY:\n",
    "                \n",
    "                # forward pass\n",
    "                b_y_hat = self.forward(bX)\n",
    "                \n",
    "                # backward pass\n",
    "                loss,delta=self.criterion_MSE(bY,b_y_hat)\n",
    "                self.backward(delta)\n",
    "                \n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x, dropout = False,p_dropout = 0.5):\n",
    "        x = np.array(x)        \n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = np.argmax(nn.forward(x[i,:],dropout = False))\n",
    "        return output\n",
    "    \n",
    "'Training the neural network'\n",
    "\n",
    "## Training the neural network\n",
    "nn = MLP([128,64,32,10], activation = 'tanh')\n",
    "\n",
    "## Start of function runtime\n",
    "model1_start_time = timeit.default_timer()\n",
    "\n",
    "## Build the model\n",
    "MSE = nn.fit(train_data, train_label, learning_rate=0.01,dropout = True, epochs=10)\n",
    "print('Cross Entropy Loss:%f'%MSE[-1])\n",
    "\n",
    "## End of function runtime\n",
    "model1_end_time = timeit.default_timer()\n",
    "\n",
    "## Runtime difference\n",
    "model1_time_dif = (model1_end_time - model1_start_time)/60\n",
    "\n",
    "## Print the time difference\n",
    "print('Function Runtime:', np.round(model1_time_dif, 2), 'minutes')\n",
    "\n",
    "'Print Test Results'\n",
    "\n",
    "## Calculating predictions and overall accuracy over testing data\n",
    "output = nn.predict(test_data, dropout = True)\n",
    "print(\"Accuracy over test data for model 1 is\",np.mean(output == label_list[42000:]))\n",
    "\n",
    "## Plotting cross entropy loss over epochs\n",
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(MSE)\n",
    "pl.grid()\n",
    "pl.title(\"Cross Entropy loss over epochs for model 1\")\n",
    "pl.xlabel(\"Epochs\")\n",
    "pl.ylabel(\"Cross Entropy loss\")\n",
    "\n",
    "## Precision, recall and F-score by class (classification report)\n",
    "model1_report = classification_report(label_list[42000:], output)\n",
    "print(model1_report)\n",
    "\n",
    "## Creating a confusion matrix\n",
    "confusion_targets = pd.Series(label_list[42000:], name='Targets')\n",
    "confusion_preds = pd.Series(output, name='Predictions')\n",
    "model1_confusion = pd.crosstab(confusion_targets, confusion_preds)\n",
    "print(model1_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: (3 Tanh, Cross-Entropy Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss:0.210628\n",
      "Function Runtime: 1.64 minutes\n",
      "Accuracy over test data for model 2 is 0.8784444444444445\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.83      0.82      1815\n",
      "          1       0.99      0.96      0.97      1823\n",
      "          2       0.82      0.79      0.81      1879\n",
      "          3       0.86      0.90      0.88      1798\n",
      "          4       0.81      0.82      0.82      1821\n",
      "          5       0.96      0.94      0.95      1782\n",
      "          6       0.71      0.68      0.70      1747\n",
      "          7       0.93      0.94      0.94      1756\n",
      "          8       0.94      0.96      0.95      1769\n",
      "          9       0.94      0.97      0.95      1810\n",
      "\n",
      "avg / total       0.88      0.88      0.88     18000\n",
      "\n",
      "Predictions   0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0\n",
      "Targets                                                                \n",
      "0            1499     3    30    91    10     4   153     0    25     0\n",
      "1               8  1755     5    44     4     0     4     0     3     0\n",
      "2              29     2  1486    26   161     3   154     0    18     0\n",
      "3              51    14    16  1611    60     0    35     0    10     1\n",
      "4               2     1   131    55  1494     2   121     0    15     0\n",
      "5               2     0     0     1     0  1676     0    58    10    35\n",
      "6             237     1   134    46   106     2  1194     1    26     0\n",
      "7               0     0     0     0     0    34     0  1648     4    70\n",
      "8               5     2     8     8     6     4    25     7  1702     2\n",
      "9               0     0     0     0     0    13     0    49     1  1747\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2101ae80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'Load the packages'\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "'Import the data'\n",
    "\n",
    "with h5py.File('train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "with h5py.File('test_128.h5','r') as H:\n",
    "    actual_test_data = np.copy(H['data'])\n",
    "    \n",
    "'Perform preprocessing' \n",
    "\n",
    "## Create a label list from the label array for prediction purposes\n",
    "label_list = []\n",
    "for i in label:\n",
    "    label_list.append(i)\n",
    "\n",
    "## Create a label matrix for computational purposes\n",
    "label_matrix = []\n",
    "for i in label:\n",
    "    vect = [0]*10\n",
    "    vect[i] = 1\n",
    "    label_matrix.append(vect)\n",
    "label_matrix = np.array(label_matrix)\n",
    "\n",
    "## Train test split\n",
    "train_data , test_data = data[:42000,:], data[42000:,:]\n",
    "train_label, test_label = label_matrix[:42000], label_matrix[42000:]\n",
    "    \n",
    "'Activation Functions'\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, x):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - x**2\n",
    "    \n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, x):\n",
    "        return  x * (1 - x)\n",
    "    \n",
    "    def __relu(self, x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def __relu_deriv(self, x):\n",
    "        return 1 * (x > 0)\n",
    "    \n",
    "    def __leaky_relu(self, x, alpha=0.01):\n",
    "        return alpha * -1 * x * (x < 0) + x * (x > 0)\n",
    "    \n",
    "    def __leaky_relu_deriv(self, x, alpha=0.01):\n",
    "        return alpha * (x < 0) + 1 * (x > 0)\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        e = np.exp(x-np.max(x))  # Stable softmax\n",
    "        if e.ndim == 1:\n",
    "            return e / np.sum(e, axis=0) # Number of dimensions 1\n",
    "        else:  \n",
    "            return e / np.array([np.sum(e, axis=1)]).T \n",
    "    \n",
    "    def __softmax_deriv(self,x):\n",
    "        e = np.exp(x-np.max(x)) \n",
    "        return e/np.sum(e) - (e/np.sum(e))**2\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.f = self.__leaky_relu\n",
    "            self.f_deriv = self.__leaky_relu_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv  \n",
    "\n",
    "'Hidden Layer Class'\n",
    "\n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out, W=None, b=None, rng = None,\n",
    "                 activation='tanh'):\n",
    "\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.activation_deriv=Activation(activation).f_deriv\n",
    "        \n",
    "        ## Initialize attentuation constant for momentum\n",
    "        global gamma\n",
    "        gamma = 0.9\n",
    "        \n",
    "        ## Rng check for dropout\n",
    "        if rng is None:\n",
    "            self.rng = np.random.RandomState(1234)\n",
    "        \n",
    "        ## Initialize weights and bias\n",
    "        self.W = np.random.normal(0, 1 ,(n_in,n_out))\n",
    "        self.b = np.zeros(n_out,)       \n",
    "        \n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "    \n",
    "        # Initialize gradients of W, b     \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        # Initialize momentum values\n",
    "        self.momentum_W = 0\n",
    "        self.momentum_b = 0\n",
    "        self.momentum_g = 0\n",
    "        self.momentum_v = 0\n",
    "        \n",
    "        # This value checks to see that the initialization of the weight norm process occurs only once\n",
    "        self.first_pass = 0\n",
    "        \n",
    "    def forward(self, input):\n",
    " \n",
    "        # Calculate linear output of initial feedforward pass\n",
    "        lin_output = np.dot(input, self.W) + self.b   \n",
    "        \n",
    "        # If initialization hasn't occurred, do so once\n",
    "        if self.first_pass == 0:\n",
    "            \n",
    "            # Calculate standard deviation and mean \n",
    "            std_lin_output = np.std(lin_output)\n",
    "            mean_lin_output = np.mean(lin_output)\n",
    "            \n",
    "            # Calculate g, v and b for initial pass\n",
    "            self.g = 1/std_lin_output\n",
    "            self.v = np.random.normal(0, 1, self.W.shape)   \n",
    "            self.b = -mean_lin_output/std_lin_output\n",
    "            \n",
    "            self.first_pass += 1\n",
    "        \n",
    "        # Calculate the norm of v\n",
    "        v_norm = np.linalg.norm(self.v)\n",
    "        \n",
    "        # Reconstruct W\n",
    "        self.W = self.g*self.v/v_norm   \n",
    "        \n",
    "        # Calculate new linear output\n",
    "        new_lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            new_lin_output if self.activation is None\n",
    "            else self.activation(new_lin_output)\n",
    "        )    \n",
    "        \n",
    "        # Pass these values to next layer\n",
    "        self.input=input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        \n",
    "        # Calculate the new gradients using momentum update\n",
    "        \n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta)) + gamma * self.momentum_W\n",
    "        self.grad_b = np.sum(delta, axis = 0) + gamma * self.momentum_b\n",
    "        v_norm = np.linalg.norm(self.v)\n",
    "        self.grad_g = self.grad_W * self.v / v_norm + gamma * self.momentum_g\n",
    "        self.grad_v = self.g * self.grad_W/v_norm - self.g*self.grad_g * self.v/(v_norm)**2 + gamma * self.momentum_v\n",
    "        delta_ = (delta.dot(self.W.T) * self.activation_deriv(self.input))\n",
    "        \n",
    "        # Return delta_ for next layer\n",
    "        return delta_\n",
    "        \n",
    "    def dropout(self, input, prob_dropout):\n",
    "\n",
    "        dropout_mask = self.rng.binomial(size=input.shape,n=1, p=1-prob_dropout)\n",
    "        return dropout_mask\n",
    "    \n",
    "'Multi-Layer Perceptron'\n",
    " \n",
    "## Create a container for dropout    \n",
    "dropout_masks = []      \n",
    "    \n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, layers,p_dropout=0.5, rng = None, activation='tanh'):\n",
    "\n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        dropout_masks = []\n",
    "        \n",
    "        if rng is None:\n",
    "            rng1 = np.random.RandomState(1234)\n",
    "        \n",
    "        self.activation=activation\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            # Hidden to Output\n",
    "            if (i % len(layers) - 2) == 0:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation='tanh'))\n",
    "            # Hidden to Hidden\n",
    "            elif (i % len(layers) - 1) == 0:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation='tanh'))\n",
    "            # Input to Hidden\n",
    "            else:\n",
    "                self.layers.append(HiddenLayer(layers[i],layers[i+1],activation='tanh'))\n",
    "            \n",
    "    def forward(self,input, dropout = False, p_dropout = 0.5):\n",
    "        for i in range(len(self.layers)):\n",
    "            output=self.layers[i].forward(input)\n",
    "            input=output\n",
    "            if dropout == True and i < len(self.layers)-1:\n",
    "                mask = self.layers[i].dropout(input = output, prob_dropout = p_dropout)\n",
    "                output *= mask\n",
    "                dropout_masks.append(mask)\n",
    "                input=output\n",
    "            self.output = output\n",
    "        return output\n",
    "    \n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation).f_deriv\n",
    "        error = (y-y_hat)\n",
    "        \n",
    "        ## Cross entropy loss\n",
    "        m = 1/y.shape[0]      \n",
    "        loss = -m * np.sum(y * np.log(np.absolute(y_hat)))\n",
    "        \n",
    "        ## divide by mini batch size\n",
    "        delta= (error/30)\n",
    "        return loss,delta      \n",
    "            \n",
    "    def backward(self,delta, dropout = False, p_dropout = 0.5):        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            delta= self.layers[i].backward(delta)\n",
    "            if dropout == True:\n",
    "                if i > 0:\n",
    "                    delta *= dropout_masks[i-1]\n",
    "        del dropout_masks[:]\n",
    "            \n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            # Calculate new momentums of W, b, g and v and update each parameter\n",
    "            layer.momentum_W = layer.momentum_W * gamma + lr * layer.grad_W\n",
    "            layer.momentum_b = layer.momentum_b * gamma + lr * layer.grad_b\n",
    "            layer.momentum_g = layer.momentum_g * gamma + lr * layer.grad_g\n",
    "            layer.momentum_v = layer.momentum_v * gamma + lr * layer.grad_v\n",
    "            layer.W += layer.momentum_W\n",
    "            layer.b += layer.momentum_b\n",
    "            layer.g += layer.momentum_g\n",
    "            layer.v += layer.momentum_v\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1,dropout = False, p_dropout = 0.5, batchsize = 30, rng = None, epochs=100):\n",
    "        \n",
    "        # Mini-batch size\n",
    "        num_of_batches = train_data.shape[0]/batchsize\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            \n",
    "            shuffled_train, shuffled_label = shuffle(train_data, train_label, random_state = 1)\n",
    "            \n",
    "            bXY = zip(np.array_split(shuffled_train, num_of_batches, axis = 0),\n",
    "                      np.array_split(shuffled_label, num_of_batches, axis = 0))\n",
    "            \n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for bX, bY in bXY:\n",
    "                \n",
    "                # forward pass\n",
    "                b_y_hat = self.forward(bX)\n",
    "                \n",
    "                # backward pass\n",
    "                loss,delta=self.criterion_MSE(bY,b_y_hat)\n",
    "                self.backward(delta)\n",
    "                \n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x, dropout = False,p_dropout = 0.5):\n",
    "        x = np.array(x)        \n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = np.argmax(nn.forward(x[i,:],dropout = False))\n",
    "        return output\n",
    "    \n",
    "'Training the neural network'\n",
    "\n",
    "## Training the neural network\n",
    "nn = MLP([128,64,32,10], activation = 'tanh')\n",
    "\n",
    "## Start of function runtime\n",
    "model2_start_time = timeit.default_timer()\n",
    "\n",
    "## Build the model\n",
    "MSE = nn.fit(train_data, train_label, learning_rate=0.01,dropout = False, epochs=100)\n",
    "print('Cross Entropy Loss:%f'%MSE[-1])\n",
    "\n",
    "## End of function runtime\n",
    "model2_end_time = timeit.default_timer()\n",
    "\n",
    "## Runtime difference\n",
    "model2_time_dif = (model2_end_time - model2_start_time)/60\n",
    "\n",
    "## Print the time difference\n",
    "print('Function Runtime:', np.round(model2_time_dif, 2), 'minutes')\n",
    "\n",
    "'Print Test Results'\n",
    "\n",
    "## Calculating predictions and overall accuracy over testing data\n",
    "output = nn.predict(test_data, dropout = False)\n",
    "print(\"Accuracy over test data for model 2 is\",np.mean(output == label_list[42000:]))\n",
    "\n",
    "## Plotting cross entropy loss over epochs\n",
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(MSE)\n",
    "pl.grid()\n",
    "pl.title(\"Cross Entropy loss over epochs for model 2\")\n",
    "pl.xlabel(\"Epochs\")\n",
    "pl.ylabel(\"Cross Entropy loss\")\n",
    "\n",
    "## Precision, recall and F-score by class (classification report)\n",
    "model2_report = classification_report(label_list[42000:], output)\n",
    "print(model2_report)\n",
    "\n",
    "## Creating a confusion matrix\n",
    "confusion_targets = pd.Series(label_list[42000:], name='Targets')\n",
    "confusion_preds = pd.Series(output, name='Predictions')\n",
    "model2_confusion = pd.crosstab(confusion_targets, confusion_preds)\n",
    "print(model2_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Test Predictions from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = nn.predict(test_data, dropout = False)\n",
    "final_labels = pd.DataFrame(final_output)\n",
    "final_labels.to_csv('predicted_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
